{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "In this study, we are interested in evaluating the performance of a Recurrent Neural Network applied to sentiment analysis. 500 positive and 500 negative reviews will be used as dataset and will be used to build a model to predict if the text in a review is a positive or negative review. We will be using pre-trained text embeddings to speed up the text analysis. We will be using 2 different text embeddings and 2 different vocabulary sizes for each embedding to understand how these parameters influence the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os  # operating system functions\n",
    "import os.path  # for manipulation of file path names\n",
    "import re  # regular expressions\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import chakin \n",
    "import time\n",
    "import warnings \n",
    "\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from sklearn.model_selection import train_test_split # Scikit Learn for random splitting of the data  \n",
    "from collections import defaultdict\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "RANDOM_SEED = 9999\n",
    "REMOVE_STOPWORDS = False  # no stopword removal \n",
    "\n",
    "CHAKIN_INDEX = 11\n",
    "NUMBER_OF_DIMENSIONS = 50\n",
    "SUBFOLDER_NAME = \"gloVe.6B\"\n",
    "\n",
    "DATA_FOLDER = \"embeddings\"\n",
    "ZIP_FILE = os.path.join(DATA_FOLDER, \"{}.zip\".format(SUBFOLDER_NAME))\n",
    "ZIP_FILE_ALT = \"glove\" + ZIP_FILE[5:]  # sometimes it's lowercase only...\n",
    "UNZIP_FOLDER = os.path.join(DATA_FOLDER, SUBFOLDER_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions definiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress warning messages\n",
    "def warn(*args, **kwargs): pass\n",
    "\n",
    "# To make output stable across runs\n",
    "def reset_graph(seed= RANDOM_SEED):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "def load_embedding_from_disks(embeddings_filename, with_indexes=True):\n",
    "    \"\"\"\n",
    "    Read a embeddings txt file. If `with_indexes=True`, \n",
    "    we return a tuple of two dictionnaries\n",
    "    `(word_to_index_dict, index_to_embedding_array)`, \n",
    "    otherwise we return only a direct \n",
    "    `word_to_embedding_dict` dictionnary mapping \n",
    "    from a string to a numpy array.\n",
    "    \"\"\"\n",
    "    if with_indexes:\n",
    "        word_to_index_dict = dict()\n",
    "        index_to_embedding_array = []\n",
    "  \n",
    "    else:\n",
    "        word_to_embedding_dict = dict()\n",
    "\n",
    "    with open(embeddings_filename, 'r', encoding='utf-8') as embeddings_file:\n",
    "        for (i, line) in enumerate(embeddings_file):\n",
    "\n",
    "            split = line.split(' ')\n",
    "\n",
    "            word = split[0]\n",
    "\n",
    "            representation = split[1:]\n",
    "            representation = np.array(\n",
    "                [float(val) for val in representation]\n",
    "            )\n",
    "\n",
    "            if with_indexes:\n",
    "                word_to_index_dict[word] = i\n",
    "                index_to_embedding_array.append(representation)\n",
    "            else:\n",
    "                word_to_embedding_dict[word] = representation\n",
    "\n",
    "    # Empty representation for unknown words.\n",
    "    _WORD_NOT_FOUND = [0.0] * len(representation)\n",
    "    if with_indexes:\n",
    "        _LAST_INDEX = i + 1\n",
    "        word_to_index_dict = defaultdict(\n",
    "            lambda: _LAST_INDEX, word_to_index_dict)\n",
    "        index_to_embedding_array = np.array(\n",
    "            index_to_embedding_array + [_WORD_NOT_FOUND])\n",
    "        return word_to_index_dict, index_to_embedding_array\n",
    "    else:\n",
    "        word_to_embedding_dict = defaultdict(lambda: _WORD_NOT_FOUND)\n",
    "        return word_to_embedding_dict\n",
    "\n",
    "# Utility function to get file names within a directory\n",
    "def listdir_no_hidden(path):\n",
    "    start_list = os.listdir(path)\n",
    "    end_list = []\n",
    "    for file in start_list:\n",
    "        if (not file.startswith('.')):\n",
    "            end_list.append(file)\n",
    "    return(end_list)\n",
    "\n",
    "# define list of codes to be dropped from document\n",
    "# carriage-returns, line-feeds, tabs\n",
    "codelist = ['\\r', '\\n', '\\t']   \n",
    "\n",
    "\n",
    "def text_parse(string):\n",
    "    # replace non-alphanumeric with space \n",
    "    temp_string = re.sub('[^a-zA-Z]', '  ', string)    \n",
    "    # replace codes with space\n",
    "    for i in range(len(codelist)):\n",
    "        stopstring = ' ' + codelist[i] + '  '\n",
    "        temp_string = re.sub(stopstring, '  ', temp_string)      \n",
    "    # replace single-character words with space\n",
    "    temp_string = re.sub('\\s.\\s', ' ', temp_string)   \n",
    "    # convert uppercase to lowercase\n",
    "    temp_string = temp_string.lower()    \n",
    "    if REMOVE_STOPWORDS:\n",
    "        # replace selected character strings/stop-words with space\n",
    "        for i in range(len(stoplist)):\n",
    "            stopstring = ' ' + str(stoplist[i]) + ' '\n",
    "            temp_string = re.sub(stopstring, ' ', temp_string)        \n",
    "    # replace multiple blank characters with one blank character\n",
    "    temp_string = re.sub('\\s+', ' ', temp_string)    \n",
    "    return(temp_string)   \n",
    "\n",
    "def default_factory():\n",
    "    return EVOCABSIZE  # last/unknown-word row in limited_index_to_embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download embeddings files\n",
    "\n",
    "We will be using GloVe embeddings and will download the English version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading embeddings to 'embeddings/gloVe.6B.zip'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100% ||                                      | Time:  0:06:28   2.1 MiB/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting embeddings to 'embeddings/gloVe.6B'\n",
      "\n",
      "Run complete\n"
     ]
    }
   ],
   "source": [
    "warnings.warn = warn\n",
    "\n",
    "if SUBFOLDER_NAME[-1] == \"d\":\n",
    "    GLOVE_FILENAME = os.path.join(\n",
    "        UNZIP_FOLDER, \"{}.txt\".format(SUBFOLDER_NAME))\n",
    "else:\n",
    "    GLOVE_FILENAME = os.path.join(UNZIP_FOLDER, \"{}.{}d.txt\".format(\n",
    "        SUBFOLDER_NAME, NUMBER_OF_DIMENSIONS))\n",
    "\n",
    "\n",
    "if not os.path.exists(ZIP_FILE) and not os.path.exists(UNZIP_FOLDER):\n",
    "    # GloVe by Stanford is licensed Apache 2.0:\n",
    "    #     https://github.com/stanfordnlp/GloVe/blob/master/LICENSE\n",
    "    #     http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
    "    #     Copyright 2014 The Board of Trustees of The Leland Stanford Junior University\n",
    "    print(\"Downloading embeddings to '{}'\".format(ZIP_FILE))\n",
    "    chakin.download(number=CHAKIN_INDEX, save_dir='./{}'.format(DATA_FOLDER))\n",
    "else:\n",
    "    print(\"Embeddings already downloaded.\")\n",
    "\n",
    "if not os.path.exists(UNZIP_FOLDER):\n",
    "    import zipfile\n",
    "    if not os.path.exists(ZIP_FILE) and os.path.exists(ZIP_FILE_ALT):\n",
    "        ZIP_FILE = ZIP_FILE_ALT\n",
    "    with zipfile.ZipFile(ZIP_FILE, \"r\") as zip_ref:\n",
    "        print(\"Extracting embeddings to '{}'\".format(UNZIP_FOLDER))\n",
    "        zip_ref.extractall(UNZIP_FOLDER)\n",
    "else:\n",
    "    print(\"Embeddings already extracted.\")\n",
    "\n",
    "print('\\nRun complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviews\n",
    "\n",
    "The training data consist of 1000 labeled reviews in text format, and we will use the 20% of them as test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/vittoriopro/Documents/AI/MSDS/422/Module8/Assignment8/run-jump-start-rnn-sentiment-v002'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Directory: movie-reviews-negative\n",
      "500 files found\n",
      "\n",
      "Processing document files under movie-reviews-negative\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------\n",
    "# gather data for 500 negative movie reviews\n",
    "# -----------------------------------------------\n",
    "dir_name = 'movie-reviews-negative'\n",
    "    \n",
    "filenames = listdir_no_hidden(path=dir_name)\n",
    "num_files = len(filenames)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
    "    assert file_exists\n",
    "print('\\nDirectory:',dir_name)    \n",
    "print('%d files found' % len(filenames))\n",
    "\n",
    "# Read data for negative movie reviews\n",
    "# Data will be stored in a list of lists where the each list represents \n",
    "# a document and document is a list of words.\n",
    "# We then break the text into words.\n",
    "\n",
    "def read_data(filename):\n",
    "\n",
    "  with open(filename, encoding='utf-8') as f:\n",
    "    data = tf.compat.as_str(f.read())\n",
    "    data = data.lower()\n",
    "    data = text_parse(data)\n",
    "    data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
    "\n",
    "  return data\n",
    "\n",
    "negative_documents = []\n",
    "\n",
    "print('\\nProcessing document files under', dir_name)\n",
    "for i in range(num_files):\n",
    "    ## print(' ', filenames[i])\n",
    "\n",
    "    words = read_data(os.path.join(dir_name, filenames[i]))\n",
    "\n",
    "    negative_documents.append(words)\n",
    "    # print('Data size (Characters) (Document %d) %d' %(i,len(words)))\n",
    "    # print('Sample string (Document %d) %s'%(i,words[:50]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Directory: movie-reviews-positive\n",
      "500 files found\n",
      "\n",
      "Processing document files under movie-reviews-positive\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------\n",
    "# gather data for 500 positive movie reviews\n",
    "# -----------------------------------------------\n",
    "dir_name = 'movie-reviews-positive'  \n",
    "filenames = listdir_no_hidden(path=dir_name)\n",
    "num_files = len(filenames)\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
    "    assert file_exists\n",
    "print('\\nDirectory:',dir_name)    \n",
    "print('%d files found' % len(filenames))\n",
    "\n",
    "# Read data for positive movie reviews\n",
    "# Data will be stored in a list of lists where the each list \n",
    "# represents a document and document is a list of words.\n",
    "# We then break the text into words.\n",
    "\n",
    "def read_data(filename):\n",
    "\n",
    "  with open(filename, encoding='utf-8') as f:\n",
    "    data = tf.compat.as_str(f.read())\n",
    "    data = data.lower()\n",
    "    data = text_parse(data)\n",
    "    data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
    "\n",
    "  return data\n",
    "\n",
    "positive_documents = []\n",
    "\n",
    "print('\\nProcessing document files under', dir_name)\n",
    "for i in range(num_files):\n",
    "    ## print(' ', filenames[i])\n",
    "\n",
    "    words = read_data(os.path.join(dir_name, filenames[i]))\n",
    "\n",
    "    positive_documents.append(words)\n",
    "    # print('Data size (Characters) (Document %d) %d' %(i,len(words)))\n",
    "    # print('Sample string (Document %d) %s'%(i,words[:50]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_review_length: 1052\n",
      "min_review_length: 22\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------\n",
    "# convert positive/negative documents into numpy array\n",
    "# note that reviews vary from 22 to 1052 words   \n",
    "# so we use the first 20 and last 20 words of each review \n",
    "# as our word sequences for analysis\n",
    "# -----------------------------------------------------\n",
    "max_review_length = 0  # initialize\n",
    "for doc in negative_documents:\n",
    "    max_review_length = max(max_review_length, len(doc))    \n",
    "for doc in positive_documents:\n",
    "    max_review_length = max(max_review_length, len(doc)) \n",
    "print('max_review_length:', max_review_length) \n",
    "\n",
    "min_review_length = max_review_length  # initialize\n",
    "for doc in negative_documents:\n",
    "    min_review_length = min(min_review_length, len(doc))    \n",
    "for doc in positive_documents:\n",
    "    min_review_length = min(min_review_length, len(doc)) \n",
    "print('min_review_length:', min_review_length) \n",
    "\n",
    "# construct list of 1000 lists with 40 words in each list\n",
    "from itertools import chain\n",
    "documents = []\n",
    "for doc in negative_documents:\n",
    "    doc_begin = doc[0:20]\n",
    "    doc_end = doc[len(doc) - 20: len(doc)]\n",
    "    documents.append(list(chain(*[doc_begin, doc_end])))    \n",
    "for doc in positive_documents:\n",
    "    doc_begin = doc[0:20]\n",
    "    doc_end = doc[len(doc) - 20: len(doc)]\n",
    "    documents.append(list(chain(*[doc_begin, doc_end])))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct list of 1000 lists with 40 words in each list\n",
    "from itertools import chain\n",
    "documents = []\n",
    "for doc in negative_documents:\n",
    "    doc_begin = doc[0:20]\n",
    "    doc_end = doc[len(doc) - 20: len(doc)]\n",
    "    documents.append(list(chain(*[doc_begin, doc_end])))    \n",
    "for doc in positive_documents:\n",
    "    doc_begin = doc[0:20]\n",
    "    doc_end = doc[len(doc) - 20: len(doc)]\n",
    "    documents.append(list(chain(*[doc_begin, doc_end])))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the labels to be used 500 negative (0) and 500 positive (1)\n",
    "thumbs_down_up = np.concatenate((np.zeros((500), dtype = np.int32), \n",
    "                      np.ones((500), dtype = np.int32)), axis = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1 and Model 2\n",
    "\n",
    "After defining metrics to evaluate de models (embedding type, number of epochs, vocabulary size, time, and test accuracy), the data of the GloVexxx is loaded and tested with the same RNN using 2 different vocabulary sizes (10.000 and 30.000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize metrics\n",
    "metrics = {}\n",
    "# Initialize metric names\n",
    "names = ['Model description', 'Number of epochs', 'Vocabulary size', 'Time in minutes', 'Training accuracy','Test accuracy']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load embeddings\n",
    "\n",
    "We will download the embedding for model 1 and 2 and will perform some checks on the data to understand vocabulary size and if correctly loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading embeddings from ./embeddings/gloVe.6B/glove.6B.50d.txt\n",
      "Embedding loaded from disks.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------- \n",
    "# Select the pre-defined embeddings source        \n",
    "# Define vocabulary size for the language model    \n",
    "# Create a word_to_embedding_dict for GloVe.6B.50d\n",
    "\n",
    "embeddings_directory = './embeddings/gloVe.6B'\n",
    "filename = 'glove.6B.50d.txt'\n",
    "embeddings_filename = os.path.join(embeddings_directory, filename)\n",
    "# ------------------------------------------------------------- \n",
    "\n",
    "print('\\nLoading embeddings from', embeddings_filename)\n",
    "word_to_index, index_to_embedding = \\\n",
    "    load_embedding_from_disks(embeddings_filename, with_indexes=True)\n",
    "print(\"Embedding loaded from disks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding is of shape: (400001, 50)\n",
      "This means (number of words, number of dimensions per word)\n",
      "\n",
      "The first words are words that tend occur more often.\n",
      "Note: for unknown words, the representation is an empty vector,\n",
      "and the index is the last one. The dictionnary has a limit:\n",
      "    A word --> Index in embedding --> Representation\n"
     ]
    }
   ],
   "source": [
    "vocab_size, embedding_dim = index_to_embedding.shape\n",
    "print(\"Embedding is of shape: {}\".format(index_to_embedding.shape))\n",
    "print(\"This means (number of words, number of dimensions per word)\\n\")\n",
    "print(\"The first words are words that tend occur more often.\")\n",
    "\n",
    "print(\"Note: for unknown words, the representation is an empty vector,\\n\"\n",
    "      \"and the index is the last one. The dictionnary has a limit:\")\n",
    "print(\"    {} --> {} --> {}\".format(\"A word\", \"Index in embedding\", \n",
    "      \"Representation\"))\n",
    "word = \"worsdfkljsdf\"  # a word obviously not in the vocabulary\n",
    "idx = word_to_index[word] # index for word obviously not in the vocabulary\n",
    "complete_vocabulary_size = idx \n",
    "embd = list(np.array(index_to_embedding[idx], dtype=int)) # \"int\" compact print\n",
    "#print(\"    {} --> {} --> {}\".format(word, idx, embd))\n",
    "word = \"the\"\n",
    "idx = word_to_index[word]\n",
    "embd = list(index_to_embedding[idx])  # \"int\" for compact print only.\n",
    "#print(\"    {} --> {} --> {}\".format(word, idx, embd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test sentence:  The quick brown fox jumps over the lazy dog \n",
      "\n",
      "Test sentence embeddings from complete vocabulary of 400000 words:\n",
      "\n",
      "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n",
      "quick:  [ 0.13967   -0.53798   -0.18047   -0.25142    0.16203   -0.13868\n",
      " -0.24637    0.75111    0.27264    0.61035   -0.82548    0.038647\n",
      " -0.32361    0.30373   -0.14598   -0.23551    0.39267   -1.1287\n",
      " -0.23636   -1.0629     0.046277   0.29143   -0.25819   -0.094902\n",
      "  0.79478   -1.2095    -0.01039   -0.092086   0.84322   -0.11061\n",
      "  3.0096     0.51652   -0.76986    0.51074    0.37508    0.12156\n",
      "  0.082794   0.43605   -0.1584    -0.61048    0.35006    0.52465\n",
      " -0.51747    0.0034705  0.73625    0.16252    0.85279    0.85268\n",
      "  0.57892    0.64483  ]\n",
      "brown:  [-0.88497   0.71685  -0.40379  -0.10698   0.81457   1.0258   -1.2698\n",
      " -0.49382  -0.27839  -0.92251  -0.49409   0.78942  -0.20066  -0.057371\n",
      "  0.060682  0.30746   0.13441  -0.49376  -0.54788  -0.81912  -0.45394\n",
      "  0.52098   1.0325   -0.8584   -0.65848  -1.2736    0.23616   1.0486\n",
      "  0.18442  -0.3901    2.1385   -0.45301  -0.16911  -0.46737   0.15938\n",
      " -0.095071 -0.26512  -0.056479  0.63849  -1.0494    0.037507  0.76434\n",
      " -0.6412   -0.59594   0.46589   0.31494  -0.34072  -0.59167  -0.31057\n",
      "  0.73274 ]\n",
      "fox:  [ 0.44206   0.059552  0.15861   0.92777   0.1876    0.24256  -1.593\n",
      " -0.79847  -0.34099  -0.24021  -0.32756   0.43639  -0.11057   0.50472\n",
      "  0.43853   0.19738  -0.1498   -0.046979 -0.83286   0.39878   0.062174\n",
      "  0.28803   0.79134   0.31798  -0.21933  -1.1015   -0.080309  0.39122\n",
      "  0.19503  -0.5936    1.7921    0.3826   -0.30509  -0.58686  -0.76935\n",
      " -0.61914  -0.61771  -0.68484  -0.67919  -0.74626  -0.036646  0.78251\n",
      " -1.0072   -0.59057  -0.7849   -0.39113  -0.49727  -0.4283   -0.15204\n",
      "  1.5064  ]\n",
      "jumps:  [-0.46105   -0.34219    0.71473   -0.29778    0.28839    0.6248\n",
      "  0.36807   -0.072746   0.60476    0.31463   -0.052247  -0.62302\n",
      " -0.56332    0.7855     0.18116   -0.31698    0.38298   -0.081953\n",
      " -1.3658    -0.78263    0.39804   -0.17001   -0.11926   -0.40146\n",
      "  1.1057    -0.51142   -0.36614    0.22177    0.34626   -0.30648\n",
      "  1.3869     0.77328    0.5946     1.2577     0.23472   -0.46087\n",
      " -0.009223   0.44534    0.012732  -0.24749   -0.7142     0.02422\n",
      "  0.083527   0.25088   -0.24259   -1.354      1.5481    -0.31728\n",
      "  0.55305   -0.0028062]\n",
      "over:  [ 0.12972    0.088073   0.24375    0.078102  -0.12783    0.27831\n",
      " -0.48693    0.19649   -0.39558   -0.28362   -0.47425   -0.59317\n",
      " -0.58804   -0.31702    0.49593    0.0087594  0.039613  -0.42495\n",
      " -0.97641   -0.46534    0.020675   0.086042   0.39317   -0.51255\n",
      " -0.17913   -1.8333     0.5622     0.41626    0.075127   0.02189\n",
      "  3.784      0.71067   -0.073943   0.15373   -0.3853    -0.070163\n",
      " -0.35374    0.074501  -0.084228  -0.45548   -0.081068   0.39157\n",
      "  0.173      0.2254    -0.12836    0.40951   -0.26079    0.090912\n",
      " -0.60515   -0.9827   ]\n",
      "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n",
      "lazy:  [-0.27611  -0.59712  -0.49227  -1.0372   -0.35878  -0.097425 -0.21014\n",
      " -0.092836 -0.054118  0.4542   -0.53296   0.37602   0.77087   0.79669\n",
      " -0.076608 -0.42515   0.42576   0.32791  -0.21996  -0.20261  -0.85139\n",
      "  0.80547   0.97621   0.9792    1.1118   -0.36062  -0.2588    0.8596\n",
      "  0.73631  -0.18601   1.2376   -0.038938  0.19246   0.52473  -0.04842\n",
      " -0.044149  0.064432  0.087822  0.42232  -0.55991  -0.44096   0.097736\n",
      " -0.17589   1.1799    0.13152  -1.0795    0.45685  -0.63312   1.2752\n",
      "  1.1672  ]\n",
      "dog:  [ 0.11008   -0.38781   -0.57615   -0.27714    0.70521    0.53994\n",
      " -1.0786    -0.40146    1.1504    -0.5678     0.0038977  0.52878\n",
      "  0.64561    0.47262    0.48549   -0.18407    0.1801     0.91397\n",
      " -1.1979    -0.5778    -0.37985    0.33606    0.772      0.75555\n",
      "  0.45506   -1.7671    -1.0503     0.42566    0.41893   -0.68327\n",
      "  1.5673     0.27685   -0.61708    0.64638   -0.076996   0.37118\n",
      "  0.1308    -0.45137    0.25398   -0.74392   -0.086199   0.24068\n",
      " -0.64819    0.83549    1.2502    -0.51379    0.04224   -0.88118\n",
      "  0.7158     0.38519  ]\n"
     ]
    }
   ],
   "source": [
    "# Show how to use embeddings dictionaries with a test sentence\n",
    "# This is a famous typing exercise with all letters of the alphabet\n",
    "# https://en.wikipedia.org/wiki/The_quick_brown_fox_jumps_over_the_lazy_dog\n",
    "a_typing_test_sentence = 'The quick brown fox jumps over the lazy dog'\n",
    "print('\\nTest sentence: ', a_typing_test_sentence, '\\n')\n",
    "words_in_test_sentence = a_typing_test_sentence.split()\n",
    "\n",
    "print('Test sentence embeddings from complete vocabulary of', \n",
    "      complete_vocabulary_size, 'words:\\n')\n",
    "for word in words_in_test_sentence:\n",
    "    word_ = word.lower()\n",
    "    embedding = index_to_embedding[word_to_index[word_]]\n",
    "    print(word_ + \": \", embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1\n",
    "\n",
    "After defining the vocabulary size for the test (10.000), the vocabulary size is reduced to the size of the n most frequently used words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10001, 50)\n",
      "\n",
      "Test sentence embeddings from vocabulary of 10000 words:\n",
      "\n",
      "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n",
      "quick:  [ 0.13967   -0.53798   -0.18047   -0.25142    0.16203   -0.13868\n",
      " -0.24637    0.75111    0.27264    0.61035   -0.82548    0.038647\n",
      " -0.32361    0.30373   -0.14598   -0.23551    0.39267   -1.1287\n",
      " -0.23636   -1.0629     0.046277   0.29143   -0.25819   -0.094902\n",
      "  0.79478   -1.2095    -0.01039   -0.092086   0.84322   -0.11061\n",
      "  3.0096     0.51652   -0.76986    0.51074    0.37508    0.12156\n",
      "  0.082794   0.43605   -0.1584    -0.61048    0.35006    0.52465\n",
      " -0.51747    0.0034705  0.73625    0.16252    0.85279    0.85268\n",
      "  0.57892    0.64483  ]\n",
      "brown:  [-0.88497   0.71685  -0.40379  -0.10698   0.81457   1.0258   -1.2698\n",
      " -0.49382  -0.27839  -0.92251  -0.49409   0.78942  -0.20066  -0.057371\n",
      "  0.060682  0.30746   0.13441  -0.49376  -0.54788  -0.81912  -0.45394\n",
      "  0.52098   1.0325   -0.8584   -0.65848  -1.2736    0.23616   1.0486\n",
      "  0.18442  -0.3901    2.1385   -0.45301  -0.16911  -0.46737   0.15938\n",
      " -0.095071 -0.26512  -0.056479  0.63849  -1.0494    0.037507  0.76434\n",
      " -0.6412   -0.59594   0.46589   0.31494  -0.34072  -0.59167  -0.31057\n",
      "  0.73274 ]\n",
      "fox:  [ 0.44206   0.059552  0.15861   0.92777   0.1876    0.24256  -1.593\n",
      " -0.79847  -0.34099  -0.24021  -0.32756   0.43639  -0.11057   0.50472\n",
      "  0.43853   0.19738  -0.1498   -0.046979 -0.83286   0.39878   0.062174\n",
      "  0.28803   0.79134   0.31798  -0.21933  -1.1015   -0.080309  0.39122\n",
      "  0.19503  -0.5936    1.7921    0.3826   -0.30509  -0.58686  -0.76935\n",
      " -0.61914  -0.61771  -0.68484  -0.67919  -0.74626  -0.036646  0.78251\n",
      " -1.0072   -0.59057  -0.7849   -0.39113  -0.49727  -0.4283   -0.15204\n",
      "  1.5064  ]\n",
      "jumps:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "over:  [ 0.12972    0.088073   0.24375    0.078102  -0.12783    0.27831\n",
      " -0.48693    0.19649   -0.39558   -0.28362   -0.47425   -0.59317\n",
      " -0.58804   -0.31702    0.49593    0.0087594  0.039613  -0.42495\n",
      " -0.97641   -0.46534    0.020675   0.086042   0.39317   -0.51255\n",
      " -0.17913   -1.8333     0.5622     0.41626    0.075127   0.02189\n",
      "  3.784      0.71067   -0.073943   0.15373   -0.3853    -0.070163\n",
      " -0.35374    0.074501  -0.084228  -0.45548   -0.081068   0.39157\n",
      "  0.173      0.2254    -0.12836    0.40951   -0.26079    0.090912\n",
      " -0.60515   -0.9827   ]\n",
      "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n",
      "lazy:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "dog:  [ 0.11008   -0.38781   -0.57615   -0.27714    0.70521    0.53994\n",
      " -1.0786    -0.40146    1.1504    -0.5678     0.0038977  0.52878\n",
      "  0.64561    0.47262    0.48549   -0.18407    0.1801     0.91397\n",
      " -1.1979    -0.5778    -0.37985    0.33606    0.772      0.75555\n",
      "  0.45506   -1.7671    -1.0503     0.42566    0.41893   -0.68327\n",
      "  1.5673     0.27685   -0.61708    0.64638   -0.076996   0.37118\n",
      "  0.1308    -0.45137    0.25398   -0.74392   -0.086199   0.24068\n",
      " -0.64819    0.83549    1.2502    -0.51379    0.04224   -0.88118\n",
      "  0.7158     0.38519  ]\n"
     ]
    }
   ],
   "source": [
    "EVOCABSIZE = 10000  # specify desired size of pre-defined embedding vocabulary \n",
    "\n",
    "vocab_size, embedding_dim = index_to_embedding.shape\n",
    "\n",
    "# dictionary has the items() function, returns list of (key, value) tuples\n",
    "limited_word_to_index = defaultdict(default_factory, \\\n",
    "    {k: v for k, v in word_to_index.items() if v < EVOCABSIZE})\n",
    "\n",
    "\n",
    "# Select the first EVOCABSIZE rows to the index_to_embedding\n",
    "limited_index_to_embedding = index_to_embedding[0:EVOCABSIZE,:]\n",
    "\n",
    "# Set the unknown-word row to be all zeros as previously\n",
    "limited_index_to_embedding = np.append(limited_index_to_embedding, \n",
    "    index_to_embedding[index_to_embedding.shape[0] - 1, :].\\\n",
    "        reshape(1,embedding_dim), \n",
    "    axis = 0)\n",
    "\n",
    "print(limited_index_to_embedding.shape)\n",
    "\n",
    "# Delete large numpy array to clear some CPU RAM\n",
    "# in this case since we eill be reusing the data for Model 2 we ill not delete it\n",
    "# del index_to_embedding\n",
    "\n",
    "# Verify the new vocabulary: should get same embeddings for test sentence\n",
    "# Note that a small EVOCABSIZE may yield some zero vectors for embeddings\n",
    "print('\\nTest sentence embeddings from vocabulary of', EVOCABSIZE, 'words:\\n')\n",
    "for word in words_in_test_sentence:\n",
    "    word_ = word.lower()\n",
    "    embedding = limited_index_to_embedding[limited_word_to_index[word_]]\n",
    "    print(word_ + \": \", embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of lists of lists for embeddings\n",
    "embeddings = []    \n",
    "for doc in documents:\n",
    "    embedding = []\n",
    "    for word in doc:\n",
    "       embedding.append(limited_index_to_embedding[limited_word_to_index[word]]) \n",
    "    embeddings.append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First word in first document: while\n",
      "Embedding for this word:\n",
      " [ 0.1011   -0.16566   0.22035  -0.10629   0.46929   0.37968  -0.62815\n",
      " -0.14385  -0.38333   0.055405  0.23511  -0.20999  -0.55395  -0.38271\n",
      "  0.21008   0.02161  -0.23054  -0.13576  -0.61636  -0.4678    0.25716\n",
      "  0.62309   0.3837   -0.25665   0.09041  -1.5184    0.4762   -0.089573\n",
      "  0.025347 -0.25974   3.6121    0.62788   0.15387  -0.062747  0.28699\n",
      " -0.16471  -0.2079    0.4407    0.065441 -0.10303  -0.15489   0.27352\n",
      "  0.38356  -0.098016  0.10705  -0.083071 -0.27168  -0.49441   0.043538\n",
      " -0.39141 ]\n",
      "Corresponding embedding from embeddings list of list of lists\n",
      " [ 0.1011   -0.16566   0.22035  -0.10629   0.46929   0.37968  -0.62815\n",
      " -0.14385  -0.38333   0.055405  0.23511  -0.20999  -0.55395  -0.38271\n",
      "  0.21008   0.02161  -0.23054  -0.13576  -0.61636  -0.4678    0.25716\n",
      "  0.62309   0.3837   -0.25665   0.09041  -1.5184    0.4762   -0.089573\n",
      "  0.025347 -0.25974   3.6121    0.62788   0.15387  -0.062747  0.28699\n",
      " -0.16471  -0.2079    0.4407    0.065441 -0.10303  -0.15489   0.27352\n",
      "  0.38356  -0.098016  0.10705  -0.083071 -0.27168  -0.49441   0.043538\n",
      " -0.39141 ]\n",
      "First word in first document: super\n",
      "Embedding for this word:\n",
      " [-0.59147    0.16468    0.18271    1.4054    -0.23347   -0.2986\n",
      " -0.34696   -0.30997   -0.089015  -0.019025   0.28963    0.46779\n",
      " -0.85615    0.68968    0.52189    0.24809   -0.022432   1.009\n",
      " -2.2903    -0.33961   -0.83609   -0.75197    0.34107    0.31885\n",
      " -0.78405   -1.2021    -0.83693   -0.28469    0.41393    0.0074962\n",
      "  1.7202     1.2959    -0.61426    0.4721     0.71448    0.55194\n",
      "  0.43352    0.35058   -1.0558    -1.2248    -0.14596    0.11694\n",
      " -0.39677    0.13791   -0.03571    1.305     -0.14112   -0.18244\n",
      "  0.22988    0.39888  ]\n",
      "Corresponding embedding from embeddings list of list of lists\n",
      " [-0.59147    0.16468    0.18271    1.4054    -0.23347   -0.2986\n",
      " -0.34696   -0.30997   -0.089015  -0.019025   0.28963    0.46779\n",
      " -0.85615    0.68968    0.52189    0.24809   -0.022432   1.009\n",
      " -2.2903    -0.33961   -0.83609   -0.75197    0.34107    0.31885\n",
      " -0.78405   -1.2021    -0.83693   -0.28469    0.41393    0.0074962\n",
      "  1.7202     1.2959    -0.61426    0.4721     0.71448    0.55194\n",
      "  0.43352    0.35058   -1.0558    -1.2248    -0.14596    0.11694\n",
      " -0.39677    0.13791   -0.03571    1.305     -0.14112   -0.18244\n",
      "  0.22988    0.39888  ]\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------    \n",
    "# Check on the embeddings list of list of lists \n",
    "# -----------------------------------------------------\n",
    "# Show the first word in the first document\n",
    "\n",
    "test_word = documents[0][0]    \n",
    "print('First word in first document:', test_word)    \n",
    "print('Embedding for this word:\\n', limited_index_to_embedding[limited_word_to_index[test_word]])\n",
    "print('Corresponding embedding from embeddings list of list of lists\\n', embeddings[0][0][:])\n",
    "\n",
    "# Show the last word in the last document\n",
    "test_word = documents[999][39]    \n",
    "print('First word in first document:', test_word)    \n",
    "print('Embedding for this word:\\n', limited_index_to_embedding[limited_word_to_index[test_word]])\n",
    "print('Corresponding embedding from embeddings list of list of lists\\n', embeddings[999][39][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------    \n",
    "# Make embeddings a numpy array for use in an RNN \n",
    "# Create training and test sets with Scikit Learn\n",
    "# -----------------------------------------------------\n",
    "embeddings_array = np.array(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random splitting of the data in to training (80%) and test (20%)  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings_array, thumbs_down_up, test_size=0.20, random_state = RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ---- Epoch  0  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.5 Test accuracy: 0.51\n",
      "\n",
      "  ---- Epoch  1  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.5 Test accuracy: 0.445\n",
      "\n",
      "  ---- Epoch  2  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.5 Test accuracy: 0.495\n",
      "\n",
      "  ---- Epoch  3  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.53 Test accuracy: 0.5\n",
      "\n",
      "  ---- Epoch  4  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.56 Test accuracy: 0.495\n",
      "\n",
      "  ---- Epoch  5  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.56 Test accuracy: 0.515\n",
      "\n",
      "  ---- Epoch  6  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.59 Test accuracy: 0.53\n",
      "\n",
      "  ---- Epoch  7  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.61 Test accuracy: 0.55\n",
      "\n",
      "  ---- Epoch  8  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.65 Test accuracy: 0.575\n",
      "\n",
      "  ---- Epoch  9  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.68 Test accuracy: 0.585\n",
      "\n",
      "  ---- Epoch  10  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.69 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  11  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.7 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  12  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.74 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  13  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.74 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  14  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.74 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  15  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.76 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  16  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.78 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  17  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.79 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  18  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.78 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  19  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.79 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  20  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.79 Test accuracy: 0.6\n",
      "\n",
      "  ---- Epoch  21  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.8 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  22  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.81 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  23  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.8 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  24  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.8 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  25  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.81 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  26  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.8 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  27  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.82 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  28  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.83 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  29  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.83 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  30  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.83 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  31  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.83 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  32  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.85 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  33  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.85 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  34  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.85 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  35  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.85 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  36  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.86 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  37  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.86 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  38  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.86 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  39  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.85 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  40  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.86 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  41  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.86 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  42  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.86 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  43  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.86 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  44  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.86 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  45  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.87 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  46  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.88 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  47  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.87 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  48  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.86 Test accuracy: 0.675\n",
      "\n",
      "  ---- Epoch  49  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.86 Test accuracy: 0.675\n",
      "Duration in minutes: 0.12\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_steps = embeddings_array.shape[1]  # number of words per document \n",
    "n_inputs = embeddings_array.shape[2]  # dimension of  pre-trained embeddings\n",
    "n_neurons = 20  # analyst specified number of neurons\n",
    "n_outputs = 2  # thumbs-down or thumbs-up\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Start timer\n",
    "start = time.clock()\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "\n",
    "logits = tf.layers.dense(states, n_outputs)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                          logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        print('\\n  ---- Epoch ', epoch, ' ----\\n')\n",
    "        for iteration in range(y_train.shape[0] // batch_size):          \n",
    "            X_batch = X_train[iteration*batch_size:(iteration + 1)*batch_size,:]\n",
    "            y_batch = y_train[iteration*batch_size:(iteration + 1)*batch_size]\n",
    "            print('  Batch ', iteration, ' training observations from ',  \n",
    "                  iteration*batch_size, ' to ', (iteration + 1)*batch_size-1,)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        print('\\n  Train accuracy:', acc_train, 'Test accuracy:', acc_test)\n",
    "        \n",
    "# Record the clock time it takes\n",
    "duration = round((time.clock() - start ) / 60, 2)\n",
    "print('Duration in minutes:', duration)\n",
    "\n",
    "#acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "\n",
    "metrics['Model 1'] = ['glove.6B.50d', n_epochs, EVOCABSIZE, duration, acc_train, acc_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2\n",
    "\n",
    "After defining the vocabulary size for the test (30.000), the vocabulary size is reduced to the size of the n most frequently used words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30001, 50)\n"
     ]
    }
   ],
   "source": [
    "# Define vocabulary size for the language model    \n",
    "# To reduce the size of the vocabulary to the n most frequently used words\n",
    "EVOCABSIZE = 30000  # specify desired size of pre-defined embedding vocabulary \n",
    "\n",
    "vocab_size, embedding_dim = index_to_embedding.shape\n",
    "\n",
    "# dictionary has the items() function, returns list of (key, value) tuples\n",
    "limited_word_to_index = defaultdict(default_factory, \\\n",
    "    {k: v for k, v in word_to_index.items() if v < EVOCABSIZE})\n",
    "\n",
    "\n",
    "# Select the first EVOCABSIZE rows to the index_to_embedding\n",
    "limited_index_to_embedding = index_to_embedding[0:EVOCABSIZE,:]\n",
    "\n",
    "# Set the unknown-word row to be all zeros as previously\n",
    "limited_index_to_embedding = np.append(limited_index_to_embedding, \n",
    "    index_to_embedding[index_to_embedding.shape[0] - 1, :].\\\n",
    "        reshape(1,embedding_dim), \n",
    "    axis = 0)\n",
    "\n",
    "print(limited_index_to_embedding.shape)\n",
    "\n",
    "# Delete large numpy array to clear some CPU RAM\n",
    "del index_to_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test sentence embeddings from vocabulary of 30000 words:\n",
      "\n",
      "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n",
      "quick:  [ 0.13967   -0.53798   -0.18047   -0.25142    0.16203   -0.13868\n",
      " -0.24637    0.75111    0.27264    0.61035   -0.82548    0.038647\n",
      " -0.32361    0.30373   -0.14598   -0.23551    0.39267   -1.1287\n",
      " -0.23636   -1.0629     0.046277   0.29143   -0.25819   -0.094902\n",
      "  0.79478   -1.2095    -0.01039   -0.092086   0.84322   -0.11061\n",
      "  3.0096     0.51652   -0.76986    0.51074    0.37508    0.12156\n",
      "  0.082794   0.43605   -0.1584    -0.61048    0.35006    0.52465\n",
      " -0.51747    0.0034705  0.73625    0.16252    0.85279    0.85268\n",
      "  0.57892    0.64483  ]\n",
      "brown:  [-0.88497   0.71685  -0.40379  -0.10698   0.81457   1.0258   -1.2698\n",
      " -0.49382  -0.27839  -0.92251  -0.49409   0.78942  -0.20066  -0.057371\n",
      "  0.060682  0.30746   0.13441  -0.49376  -0.54788  -0.81912  -0.45394\n",
      "  0.52098   1.0325   -0.8584   -0.65848  -1.2736    0.23616   1.0486\n",
      "  0.18442  -0.3901    2.1385   -0.45301  -0.16911  -0.46737   0.15938\n",
      " -0.095071 -0.26512  -0.056479  0.63849  -1.0494    0.037507  0.76434\n",
      " -0.6412   -0.59594   0.46589   0.31494  -0.34072  -0.59167  -0.31057\n",
      "  0.73274 ]\n",
      "fox:  [ 0.44206   0.059552  0.15861   0.92777   0.1876    0.24256  -1.593\n",
      " -0.79847  -0.34099  -0.24021  -0.32756   0.43639  -0.11057   0.50472\n",
      "  0.43853   0.19738  -0.1498   -0.046979 -0.83286   0.39878   0.062174\n",
      "  0.28803   0.79134   0.31798  -0.21933  -1.1015   -0.080309  0.39122\n",
      "  0.19503  -0.5936    1.7921    0.3826   -0.30509  -0.58686  -0.76935\n",
      " -0.61914  -0.61771  -0.68484  -0.67919  -0.74626  -0.036646  0.78251\n",
      " -1.0072   -0.59057  -0.7849   -0.39113  -0.49727  -0.4283   -0.15204\n",
      "  1.5064  ]\n",
      "jumps:  [-0.46105   -0.34219    0.71473   -0.29778    0.28839    0.6248\n",
      "  0.36807   -0.072746   0.60476    0.31463   -0.052247  -0.62302\n",
      " -0.56332    0.7855     0.18116   -0.31698    0.38298   -0.081953\n",
      " -1.3658    -0.78263    0.39804   -0.17001   -0.11926   -0.40146\n",
      "  1.1057    -0.51142   -0.36614    0.22177    0.34626   -0.30648\n",
      "  1.3869     0.77328    0.5946     1.2577     0.23472   -0.46087\n",
      " -0.009223   0.44534    0.012732  -0.24749   -0.7142     0.02422\n",
      "  0.083527   0.25088   -0.24259   -1.354      1.5481    -0.31728\n",
      "  0.55305   -0.0028062]\n",
      "over:  [ 0.12972    0.088073   0.24375    0.078102  -0.12783    0.27831\n",
      " -0.48693    0.19649   -0.39558   -0.28362   -0.47425   -0.59317\n",
      " -0.58804   -0.31702    0.49593    0.0087594  0.039613  -0.42495\n",
      " -0.97641   -0.46534    0.020675   0.086042   0.39317   -0.51255\n",
      " -0.17913   -1.8333     0.5622     0.41626    0.075127   0.02189\n",
      "  3.784      0.71067   -0.073943   0.15373   -0.3853    -0.070163\n",
      " -0.35374    0.074501  -0.084228  -0.45548   -0.081068   0.39157\n",
      "  0.173      0.2254    -0.12836    0.40951   -0.26079    0.090912\n",
      " -0.60515   -0.9827   ]\n",
      "the:  [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n",
      "lazy:  [-0.27611  -0.59712  -0.49227  -1.0372   -0.35878  -0.097425 -0.21014\n",
      " -0.092836 -0.054118  0.4542   -0.53296   0.37602   0.77087   0.79669\n",
      " -0.076608 -0.42515   0.42576   0.32791  -0.21996  -0.20261  -0.85139\n",
      "  0.80547   0.97621   0.9792    1.1118   -0.36062  -0.2588    0.8596\n",
      "  0.73631  -0.18601   1.2376   -0.038938  0.19246   0.52473  -0.04842\n",
      " -0.044149  0.064432  0.087822  0.42232  -0.55991  -0.44096   0.097736\n",
      " -0.17589   1.1799    0.13152  -1.0795    0.45685  -0.63312   1.2752\n",
      "  1.1672  ]\n",
      "dog:  [ 0.11008   -0.38781   -0.57615   -0.27714    0.70521    0.53994\n",
      " -1.0786    -0.40146    1.1504    -0.5678     0.0038977  0.52878\n",
      "  0.64561    0.47262    0.48549   -0.18407    0.1801     0.91397\n",
      " -1.1979    -0.5778    -0.37985    0.33606    0.772      0.75555\n",
      "  0.45506   -1.7671    -1.0503     0.42566    0.41893   -0.68327\n",
      "  1.5673     0.27685   -0.61708    0.64638   -0.076996   0.37118\n",
      "  0.1308    -0.45137    0.25398   -0.74392   -0.086199   0.24068\n",
      " -0.64819    0.83549    1.2502    -0.51379    0.04224   -0.88118\n",
      "  0.7158     0.38519  ]\n"
     ]
    }
   ],
   "source": [
    "# Verify the new vocabulary: should get same embeddings for test sentence\n",
    "# Note that a small EVOCABSIZE may yield some zero vectors for embeddings\n",
    "print('\\nTest sentence embeddings from vocabulary of', EVOCABSIZE, 'words:\\n')\n",
    "for word in words_in_test_sentence:\n",
    "    word_ = word.lower()\n",
    "    embedding = limited_index_to_embedding[limited_word_to_index[word_]]\n",
    "    print(word_ + \": \", embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of lists of lists for embeddings\n",
    "embeddings = []    \n",
    "for doc in documents:\n",
    "    embedding = []\n",
    "    for word in doc:\n",
    "       embedding.append(limited_index_to_embedding[limited_word_to_index[word]]) \n",
    "    embeddings.append(embedding)\n",
    "    \n",
    "# -----------------------------------------------------    \n",
    "# Make embeddings a numpy array for use in an RNN \n",
    "# Create training and test sets with Scikit Learn\n",
    "# -----------------------------------------------------\n",
    "embeddings_array = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random splitting of the data in to training (80%) and test (20%)  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings_array, thumbs_down_up, test_size=0.20, random_state = RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ---- Epoch  0  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.5 Test accuracy: 0.485\n",
      "\n",
      "  ---- Epoch  1  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.5 Test accuracy: 0.44\n",
      "\n",
      "  ---- Epoch  2  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.53 Test accuracy: 0.48\n",
      "\n",
      "  ---- Epoch  3  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.53 Test accuracy: 0.495\n",
      "\n",
      "  ---- Epoch  4  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.55 Test accuracy: 0.51\n",
      "\n",
      "  ---- Epoch  5  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.57 Test accuracy: 0.53\n",
      "\n",
      "  ---- Epoch  6  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.57 Test accuracy: 0.545\n",
      "\n",
      "  ---- Epoch  7  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.59 Test accuracy: 0.56\n",
      "\n",
      "  ---- Epoch  8  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.61 Test accuracy: 0.555\n",
      "\n",
      "  ---- Epoch  9  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.62 Test accuracy: 0.585\n",
      "\n",
      "  ---- Epoch  10  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.63 Test accuracy: 0.59\n",
      "\n",
      "  ---- Epoch  11  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.63 Test accuracy: 0.585\n",
      "\n",
      "  ---- Epoch  12  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.64 Test accuracy: 0.6\n",
      "\n",
      "  ---- Epoch  13  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.66 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  14  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.67 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  15  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.67 Test accuracy: 0.61\n",
      "\n",
      "  ---- Epoch  16  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.68 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  17  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.73 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  18  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.73 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  19  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.73 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  20  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.78 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  21  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.79 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  22  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.79 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  23  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.79 Test accuracy: 0.675\n",
      "\n",
      "  ---- Epoch  24  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.79 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  25  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.81 Test accuracy: 0.69\n",
      "\n",
      "  ---- Epoch  26  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.82 Test accuracy: 0.68\n",
      "\n",
      "  ---- Epoch  27  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.8 Test accuracy: 0.69\n",
      "\n",
      "  ---- Epoch  28  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.82 Test accuracy: 0.68\n",
      "\n",
      "  ---- Epoch  29  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.81 Test accuracy: 0.69\n",
      "\n",
      "  ---- Epoch  30  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.83 Test accuracy: 0.7\n",
      "\n",
      "  ---- Epoch  31  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.84 Test accuracy: 0.695\n",
      "\n",
      "  ---- Epoch  32  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.83 Test accuracy: 0.69\n",
      "\n",
      "  ---- Epoch  33  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.82 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  34  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.81 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  35  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.81 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  36  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.8 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  37  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.84 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  38  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.84 Test accuracy: 0.68\n",
      "\n",
      "  ---- Epoch  39  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.84 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  40  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.84 Test accuracy: 0.68\n",
      "\n",
      "  ---- Epoch  41  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.83 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  42  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.85 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  43  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.85 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  44  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.85 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  45  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.86 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  46  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.85 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  47  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.86 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  48  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.85 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  49  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.88 Test accuracy: 0.655\n",
      "Duration in minutes: 0.12\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_steps = embeddings_array.shape[1]  # number of words per document \n",
    "n_inputs = embeddings_array.shape[2]  # dimension of  pre-trained embeddings\n",
    "n_neurons = 20  # analyst specified number of neurons\n",
    "n_outputs = 2  # thumbs-down or thumbs-up\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Start timer\n",
    "start = time.clock()\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "\n",
    "logits = tf.layers.dense(states, n_outputs)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                          logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        print('\\n  ---- Epoch ', epoch, ' ----\\n')\n",
    "        for iteration in range(y_train.shape[0] // batch_size):          \n",
    "            X_batch = X_train[iteration*batch_size:(iteration + 1)*batch_size,:]\n",
    "            y_batch = y_train[iteration*batch_size:(iteration + 1)*batch_size]\n",
    "            print('  Batch ', iteration, ' training observations from ',  \n",
    "                  iteration*batch_size, ' to ', (iteration + 1)*batch_size-1,)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        print('\\n  Train accuracy:', acc_train, 'Test accuracy:', acc_test)\n",
    "        \n",
    "# Record the clock time it takes\n",
    "duration = round((time.clock() - start ) / 60, 2)\n",
    "print('Duration in minutes:', duration)\n",
    "\n",
    "metrics['Model 2'] = ['glove.6B.50d', n_epochs, EVOCABSIZE, duration,  acc_train, acc_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3 and Model 4\n",
    "\n",
    "For model 3 and model 4, the data of the Glove6B300d is loaded and tested with the same RNN, using 2 different vocabulary sizes (10.000 and 30.000)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load embeddings \n",
    "\n",
    "We will download the embedding for model 3 and 3 and will perform some checks on the data to understand vocabulary size and if correctly loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading embeddings from ./embeddings/gloVe.6B/glove.6B.300d.txt\n",
      "Embedding loaded from disks.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------- \n",
    "# Select the pre-defined embeddings source        \n",
    "# Define vocabulary size for the language model    \n",
    "# Create a word_to_embedding_dict for GloVe.6B.50d\n",
    "\n",
    "embeddings_directory = './embeddings/gloVe.6B'\n",
    "filename = 'glove.6B.300d.txt'\n",
    "embeddings_filename = os.path.join(embeddings_directory, filename)\n",
    "# ------------------------------------------------------------- \n",
    "\n",
    "print('\\nLoading embeddings from', embeddings_filename)\n",
    "word_to_index, index_to_embedding = \\\n",
    "    load_embedding_from_disks(embeddings_filename, with_indexes=True)\n",
    "print(\"Embedding loaded from disks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding is of shape: (400001, 300)\n",
      "This means (number of words, number of dimensions per word)\n",
      "\n",
      "The first words are words that tend occur more often.\n",
      "Note: for unknown words, the representation is an empty vector,\n",
      "and the index is the last one. The dictionnary has a limit:\n",
      "    A word --> Index in embedding --> Representation\n"
     ]
    }
   ],
   "source": [
    "vocab_size, embedding_dim = index_to_embedding.shape\n",
    "print(\"Embedding is of shape: {}\".format(index_to_embedding.shape))\n",
    "print(\"This means (number of words, number of dimensions per word)\\n\")\n",
    "print(\"The first words are words that tend occur more often.\")\n",
    "\n",
    "print(\"Note: for unknown words, the representation is an empty vector,\\n\"\n",
    "      \"and the index is the last one. The dictionnary has a limit:\")\n",
    "print(\"    {} --> {} --> {}\".format(\"A word\", \"Index in embedding\", \n",
    "      \"Representation\"))\n",
    "word = \"worsdfkljsdf\"  # a word obviously not in the vocabulary\n",
    "idx = word_to_index[word] # index for word obviously not in the vocabulary\n",
    "complete_vocabulary_size = idx \n",
    "embd = list(np.array(index_to_embedding[idx], dtype=int)) # \"int\" compact print\n",
    "#print(\"    {} --> {} --> {}\".format(word, idx, embd))\n",
    "word = \"the\"\n",
    "idx = word_to_index[word]\n",
    "embd = list(index_to_embedding[idx])  # \"int\" for compact print only.\n",
    "#print(\"    {} --> {} --> {}\".format(word, idx, embd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3\n",
    "\n",
    "After defining the vocabulary size for the test (10.000), the vocabulary size is reduced to the size of the n most frequently used words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10001, 300)\n"
     ]
    }
   ],
   "source": [
    "EVOCABSIZE = 10000  # specify desired size of pre-defined embedding vocabulary \n",
    "\n",
    "vocab_size, embedding_dim = index_to_embedding.shape\n",
    "\n",
    "# dictionary has the items() function, returns list of (key, value) tuples\n",
    "limited_word_to_index = defaultdict(default_factory, \\\n",
    "    {k: v for k, v in word_to_index.items() if v < EVOCABSIZE})\n",
    "\n",
    "\n",
    "# Select the first EVOCABSIZE rows to the index_to_embedding\n",
    "limited_index_to_embedding = index_to_embedding[0:EVOCABSIZE,:]\n",
    "\n",
    "# Set the unknown-word row to be all zeros as previously\n",
    "limited_index_to_embedding = np.append(limited_index_to_embedding, \n",
    "    index_to_embedding[index_to_embedding.shape[0] - 1, :].\\\n",
    "        reshape(1,embedding_dim), \n",
    "    axis = 0)\n",
    "\n",
    "print(limited_index_to_embedding.shape)\n",
    "\n",
    "# Delete large numpy array to clear some CPU RAM\n",
    "# in this case since we eill be reusing the data for Model 4 we ill not delete it\n",
    "\n",
    "#del index_to_embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test sentence embeddings from vocabulary of 10000 words:\n",
      "\n",
      "the:  [ 4.6560e-02  2.1318e-01 -7.4364e-03 -4.5854e-01 -3.5639e-02  2.3643e-01\n",
      " -2.8836e-01  2.1521e-01 -1.3486e-01 -1.6413e+00 -2.6091e-01  3.2434e-02\n",
      "  5.6621e-02 -4.3296e-02 -2.1672e-02  2.2476e-01 -7.5129e-02 -6.7018e-02\n",
      " -1.4247e-01  3.8825e-02 -1.8951e-01  2.9977e-01  3.9305e-01  1.7887e-01\n",
      " -1.7343e-01 -2.1178e-01  2.3617e-01 -6.3681e-02 -4.2318e-01 -1.1661e-01\n",
      "  9.3754e-02  1.7296e-01 -3.3073e-01  4.9112e-01 -6.8995e-01 -9.2462e-02\n",
      "  2.4742e-01 -1.7991e-01  9.7908e-02  8.3118e-02  1.5299e-01 -2.7276e-01\n",
      " -3.8934e-02  5.4453e-01  5.3737e-01  2.9105e-01 -7.3514e-03  4.7880e-02\n",
      " -4.0760e-01 -2.6759e-02  1.7919e-01  1.0977e-02 -1.0963e-01 -2.6395e-01\n",
      "  7.3990e-02  2.6236e-01 -1.5080e-01  3.4623e-01  2.5758e-01  1.1971e-01\n",
      " -3.7135e-02 -7.1593e-02  4.3898e-01 -4.0764e-02  1.6425e-02 -4.4640e-01\n",
      "  1.7197e-01  4.6246e-02  5.8639e-02  4.1499e-02  5.3948e-01  5.2495e-01\n",
      "  1.1361e-01 -4.8315e-02 -3.6385e-01  1.8704e-01  9.2761e-02 -1.1129e-01\n",
      " -4.2085e-01  1.3992e-01 -3.9338e-01 -6.7945e-02  1.2188e-01  1.6707e-01\n",
      "  7.5169e-02 -1.5529e-02 -1.9499e-01  1.9638e-01  5.3194e-02  2.5170e-01\n",
      " -3.4845e-01 -1.0638e-01 -3.4692e-01 -1.9024e-01 -2.0040e-01  1.2154e-01\n",
      " -2.9208e-01  2.3353e-02 -1.1618e-01 -3.5768e-01  6.2304e-02  3.5884e-01\n",
      "  2.9060e-02  7.3005e-03  4.9482e-03 -1.5048e-01 -1.2313e-01  1.9337e-01\n",
      "  1.2173e-01  4.4503e-01  2.5147e-01  1.0781e-01 -1.7716e-01  3.8691e-02\n",
      "  8.1530e-02  1.4667e-01  6.3666e-02  6.1332e-02 -7.5569e-02 -3.7724e-01\n",
      "  1.5850e-02 -3.0342e-01  2.8374e-01 -4.2013e-02 -4.0715e-02 -1.5269e-01\n",
      "  7.4980e-02  1.5577e-01  1.0433e-01  3.1393e-01  1.9309e-01  1.9429e-01\n",
      "  1.5185e-01 -1.0192e-01 -1.8785e-02  2.0791e-01  1.3366e-01  1.9038e-01\n",
      " -2.5558e-01  3.0400e-01 -1.8960e-02  2.0147e-01 -4.2110e-01 -7.5156e-03\n",
      " -2.7977e-01 -1.9314e-01  4.6204e-02  1.9971e-01 -3.0207e-01  2.5735e-01\n",
      "  6.8107e-01 -1.9409e-01  2.3984e-01  2.2493e-01  6.5224e-01 -1.3561e-01\n",
      " -1.7383e-01 -4.8209e-02 -1.1860e-01  2.1588e-03 -1.9525e-02  1.1948e-01\n",
      "  1.9346e-01 -4.0820e-01 -8.2966e-02  1.6626e-01 -1.0601e-01  3.5861e-01\n",
      "  1.6922e-01  7.2590e-02 -2.4803e-01 -1.0024e-01 -5.2491e-01 -1.7745e-01\n",
      " -3.6647e-01  2.6180e-01 -1.2077e-02  8.3190e-02 -2.1528e-01  4.1045e-01\n",
      "  2.9136e-01  3.0869e-01  7.8864e-02  3.2207e-01 -4.1023e-02 -1.0970e-01\n",
      " -9.2041e-02 -1.2339e-01 -1.6416e-01  3.5382e-01 -8.2774e-02  3.3171e-01\n",
      " -2.4738e-01 -4.8928e-02  1.5746e-01  1.8988e-01 -2.6642e-02  6.3315e-02\n",
      " -1.0673e-02  3.4089e-01  1.4106e+00  1.3417e-01  2.8191e-01 -2.5940e-01\n",
      "  5.5267e-02 -5.2425e-02 -2.5789e-01  1.9127e-02 -2.2084e-02  3.2113e-01\n",
      "  6.8818e-02  5.1207e-01  1.6478e-01 -2.0194e-01  2.9232e-01  9.8575e-02\n",
      "  1.3145e-02 -1.0652e-01  1.3510e-01 -4.5332e-02  2.0697e-01 -4.8425e-01\n",
      " -4.4706e-01  3.3305e-03  2.9264e-03 -1.0975e-01 -2.3325e-01  2.2442e-01\n",
      " -1.0503e-01  1.2339e-01  1.0978e-01  4.8994e-02 -2.5157e-01  4.0319e-01\n",
      "  3.5318e-01  1.8651e-01 -2.3622e-02 -1.2734e-01  1.1475e-01  2.7359e-01\n",
      " -2.1866e-01  1.5794e-02  8.1754e-01 -2.3792e-02 -8.5469e-01 -1.6203e-01\n",
      "  1.8076e-01  2.8014e-02 -1.4340e-01  1.3139e-03 -9.1735e-02 -8.9704e-02\n",
      "  1.1105e-01 -1.6703e-01  6.8377e-02 -8.7388e-02 -3.9789e-02  1.4184e-02\n",
      "  2.1187e-01  2.8579e-01 -2.8797e-01 -5.8996e-02 -3.2436e-02 -4.7009e-03\n",
      " -1.7052e-01 -3.4741e-02 -1.1489e-01  7.5093e-02  9.9526e-02  4.8183e-02\n",
      " -7.3775e-02 -4.1817e-01  4.1268e-03  4.4414e-01 -1.6062e-01  1.4294e-01\n",
      " -2.2628e+00 -2.7347e-02  8.1311e-01  7.7417e-01 -2.5639e-01 -1.1576e-01\n",
      " -1.1982e-01 -2.1363e-01  2.8429e-02  2.7261e-01  3.1026e-02  9.6782e-02\n",
      "  6.7769e-03  1.4082e-01 -1.3064e-02 -2.9686e-01 -7.9913e-02  1.9500e-01\n",
      "  3.1549e-02  2.8506e-01 -8.7461e-02  9.0611e-03 -2.0989e-01  5.3913e-02]\n",
      "quick:  [ 0.37594    0.063183   0.51835   -0.48652    0.34026   -0.17801\n",
      "  0.32615   -0.32989    0.25508   -1.026     -0.13841    0.27258\n",
      " -0.010244   0.35186    0.28341    0.3189    -0.18892   -0.292\n",
      " -0.071297   0.25631   -0.34286    0.16179    0.065725  -0.038052\n",
      "  0.1457     0.20289    0.14274   -0.15024    0.35412   -0.13715\n",
      " -0.2677    -0.011243  -0.1541     0.1765    -1.5424     0.37699\n",
      " -0.28239    0.19172   -0.46349   -0.26345   -0.39337    0.41276\n",
      "  0.42873   -0.1317     0.31096   -0.28715    0.47212   -0.41866\n",
      " -0.11871   -0.026621  -0.21089   -0.074757   0.26429   -0.032246\n",
      " -0.084233   0.48653   -0.38692   -0.3756    -0.27031    0.15208\n",
      "  0.2263    -0.28777    0.12475   -0.17029    0.019322   0.34175\n",
      "  0.69492   -0.42039    0.3161    -0.23871    0.30485   -0.078005\n",
      "  0.25937   -0.036324   0.27365   -0.1698    -0.10004    0.13614\n",
      "  0.0094279 -0.51261    0.3464     0.032112   0.25292    0.1947\n",
      "  0.16906   -0.044683  -0.039252   0.31651   -0.27185    0.10862\n",
      "  0.070371   0.31916   -0.55993   -0.5553     0.5493    -0.17244\n",
      " -0.70848    0.039063   0.33553   -0.11393   -0.28882   -0.53623\n",
      "  0.0021584  0.24971    0.31383   -0.2516    -0.28619    0.20113\n",
      " -0.29545   -0.3285     0.33289    0.19422    0.047601  -0.13157\n",
      "  0.4269     0.085041  -0.30294   -0.38344   -0.035083  -0.0463\n",
      "  0.035454  -0.052446   0.51216   -0.37809   -0.24834    0.28464\n",
      "  0.019408   0.61137    0.14859    0.30104    0.23773    0.37627\n",
      " -0.64467    0.19701   -0.19264   -0.013601   0.073281  -0.43031\n",
      "  0.38081   -0.42172   -0.16131    0.12108   -0.12078   -0.20818\n",
      " -0.4697     0.1279    -0.63088    0.16412    0.20474    0.16701\n",
      " -0.79632   -0.075741  -0.25251   -0.025189   0.081245  -0.081758\n",
      " -0.12925   -0.33034    0.039839  -0.30436    0.023003  -0.35589\n",
      "  0.40923   -0.10969   -0.084268   0.56261    0.37604    0.10676\n",
      " -0.1678     0.11219   -0.13141   -0.025916  -0.56102   -0.074779\n",
      " -0.14769    0.13028   -0.38351    0.055938   0.1996     0.0052525\n",
      "  0.11655   -0.58141    0.45407   -0.11067   -0.10262    0.31478\n",
      " -0.049739  -0.34926   -0.016468  -0.12476   -0.071381   0.34803\n",
      " -0.12247   -0.38406    0.095986   0.12451   -0.033609  -0.62353\n",
      "  0.25048   -0.1427     0.60613   -0.080829   0.25008    0.059055\n",
      " -0.17486    0.14913   -0.41488    0.27573   -0.11921   -0.02267\n",
      " -0.34188   -0.49563   -0.22119    0.49553   -0.035482  -0.11908\n",
      "  0.0096008 -0.44059   -0.35947   -0.19156    0.28505    0.35236\n",
      " -0.3384    -0.28643   -0.22068   -0.29761    0.10412   -0.067384\n",
      "  0.043089  -0.05794   -0.31212    0.24026    0.072559  -0.024896\n",
      " -0.19299    0.020044  -0.10826    0.2022     0.097076   0.43886\n",
      "  0.35085    0.38611   -0.1838    -0.047166  -0.5351    -0.17215\n",
      "  0.17407    0.17959   -0.35965   -0.23817    0.16348   -0.79488\n",
      "  0.18858    0.027404   0.23823    0.047581   0.2485     0.18332\n",
      " -0.22626    0.54554   -0.31324   -0.22699   -0.31341    0.68296\n",
      "  0.13422   -0.27644   -0.38901   -0.29207   -0.10058    0.12057\n",
      " -0.36691   -0.69507   -0.22242   -0.023121   0.72283    0.0051197\n",
      " -1.7769     0.40651    0.025966  -0.18157    0.23957    0.37943\n",
      "  0.67713    0.49789    0.3634    -0.60131    0.53868   -0.18682\n",
      " -0.14783    0.40581    0.1379     0.054337  -0.12388    0.064828\n",
      "  0.27453    0.5165    -0.1955    -0.55939   -0.2744    -0.12146  ]\n",
      "brown:  [ 0.2793     0.18372   -0.11257    0.21734   -0.21657   -0.50335\n",
      " -0.27194    0.32181    0.031892  -0.37998    0.15544   -0.32953\n",
      " -0.19827    0.20403    0.26768    0.292     -0.34187   -0.10766\n",
      " -0.43697   -0.14488    0.14634    0.21591    0.12576    0.14895\n",
      " -0.21763    0.030797   0.10949   -0.41689   -0.30296   -0.14592\n",
      " -0.56228    0.33282   -0.20436   -0.24403   -1.4732     0.68345\n",
      "  0.45336    0.43671   -0.15641    0.15075   -0.24265   -0.040059\n",
      "  0.22323    0.19523    0.37445   -0.18509   -0.10302   -0.055363\n",
      " -0.17274   -0.45401   -0.14729   -0.24133   -0.043826  -0.23243\n",
      "  0.42367    0.15906   -0.14039   -0.36185   -0.26695   -0.42724\n",
      " -0.08843   -0.099597   0.24257   -0.05424    0.10746   -1.1304\n",
      "  0.024651  -0.10212    0.046319  -0.68792    0.4214    -0.25844\n",
      "  0.17052    0.097878   0.026835   0.32044    0.0062988  0.24575\n",
      "  0.20126   -0.16771    0.19825    0.28939   -0.064994  -0.38766\n",
      "  0.52509    0.38195    0.32421    0.20683   -0.48472   -0.080334\n",
      " -0.15345    0.35459   -0.43765    0.071575  -0.39516   -0.22906\n",
      "  0.25686    0.26659    0.37626   -0.18556    0.16445   -0.33614\n",
      " -0.56262    0.067852  -0.61642    0.19546    0.45027    0.20238\n",
      "  0.33957    0.41372    0.11855    0.087619   0.18754    0.17901\n",
      "  0.022569  -0.10854   -0.47226    0.41039    0.32588   -0.58468\n",
      " -0.0057296 -0.29201   -0.12777   -0.15729   -0.40103   -0.039414\n",
      " -0.1192     0.40093    0.032862   0.39862   -0.63525    0.11594\n",
      " -0.39954    0.36919   -0.50021   -0.51169   -0.13955    0.18055\n",
      " -0.079918  -0.19474    0.53131    0.093723   0.2773    -0.40505\n",
      " -0.20568    0.11139    0.032661  -0.04852    0.44576    0.23667\n",
      "  0.54981    0.23585   -0.51539   -0.46424    0.021099  -0.3919\n",
      "  0.58338   -0.89908    0.094066   0.30159   -0.063199  -0.31635\n",
      "  0.50333   -0.068517  -0.38681    0.33      -0.49463    0.75491\n",
      " -0.088266  -0.19413    0.4238    -0.031727  -0.4464    -0.21028\n",
      " -0.11151   -0.07088   -0.027832  -0.63304    0.27336   -0.47925\n",
      " -0.03239    0.46069    0.16968   -0.38262   -0.31413   -0.29068\n",
      " -0.031801  -0.48974   -0.50999    0.1466     0.0027995  0.56333\n",
      " -0.044347  -0.085679   0.20559   -0.051593   0.75228   -0.013291\n",
      " -0.084694  -0.4305     1.1734    -0.083233   0.1561    -0.15758\n",
      "  0.19066   -0.2966     0.63704    0.45616   -0.34797   -0.12732\n",
      "  0.4901    -0.51217   -0.063474  -0.061496   0.28825    0.17711\n",
      "  0.46301   -0.12697   -0.044627  -1.0064     0.76394    0.20494\n",
      "  0.028766   0.27597    0.021726  -0.12054    0.23284    0.18999\n",
      "  0.30048   -0.056139   0.09546   -0.036514   0.0084885  0.016599\n",
      " -0.31428   -0.2707     0.099281   0.4445    -0.36      -0.55556\n",
      " -0.18551   -0.30644    0.056475  -0.19197   -0.48886    0.33044\n",
      "  0.19535   -0.53828    0.12385   -0.29372   -0.1036     0.0051129\n",
      "  0.11483   -0.10591    0.73337    0.26978   -0.06925    0.11565\n",
      "  0.27711    0.15109   -0.069137  -0.14481    0.32319    0.039345\n",
      " -0.44964    0.27103    0.045326  -0.064534  -0.37144    0.47615\n",
      " -0.61105   -0.11922   -0.068806   0.15401   -0.40812    0.32575\n",
      " -1.2888     0.0203    -0.12893   -0.22211   -0.16402    0.29018\n",
      "  0.36295   -0.081025  -0.50492    0.5046    -0.37485    0.52111\n",
      "  0.1757     0.069686   0.48937   -0.17747   -0.20577    0.70419\n",
      "  0.068633   0.47878   -0.21754   -0.016868  -0.91378    0.45643  ]\n",
      "fox:  [-1.1570e-01 -2.5048e-02 -1.1013e-01 -4.8060e-02 -8.5504e-02  3.7308e-01\n",
      " -9.4790e-01  5.9662e-01 -2.0006e-02 -3.0469e-01  2.4997e-01 -2.0005e-01\n",
      "  1.9284e-01  4.7197e-01  2.7099e-01  3.4190e-01 -2.9186e-01 -3.9718e-01\n",
      "  5.5653e-01  3.2774e-01  9.4860e-02 -4.6588e-01  6.2119e-01  2.0709e-01\n",
      "  8.2332e-02 -1.0175e-01  3.2067e-01 -3.1875e-01  1.7941e-01 -2.4127e-01\n",
      " -1.7961e-02 -8.7154e-02  3.3423e-01 -4.5026e-02 -1.2193e+00  1.9321e-01\n",
      "  2.1427e-01  2.3893e-01 -1.8084e-01 -1.5920e-01 -1.0387e-01  2.2060e-01\n",
      "  2.5423e-01  2.0815e-01 -3.3072e-01 -2.0672e-01 -3.1424e-01  2.6498e-02\n",
      "  2.1955e-01 -3.1157e-01 -7.8914e-02 -1.5089e-02 -4.0848e-02 -2.3807e-01\n",
      "  3.1983e-01 -7.4173e-02  2.2983e-01  5.5735e-02  5.6743e-03 -7.7368e-01\n",
      "  2.7412e-01 -1.8999e-01  5.3825e-01 -1.9784e-01 -1.0898e-01 -3.4022e-01\n",
      "  3.1582e-01  5.0317e-01  3.3955e-01 -2.7143e-01 -3.3026e-02  3.4279e-01\n",
      " -4.9815e-01  2.5056e-01 -1.2791e-01 -1.5912e-01 -8.7541e-03  1.0450e-01\n",
      "  2.1969e-01 -5.9077e-02 -3.7600e-01  6.0835e-02  1.8427e-01 -8.3592e-02\n",
      "  1.1632e-01  7.5145e-01  6.5186e-02 -3.3258e-01  1.4249e-01 -6.5348e-01\n",
      " -2.6661e-03  1.0233e-01 -3.8731e-01  6.0529e-01 -2.7326e-01 -3.5777e-01\n",
      " -5.3533e-01  4.9589e-02  2.0224e-01 -3.2805e-01  3.9305e-01 -2.7540e-01\n",
      " -1.3042e-01  9.5833e-01  3.8853e-01  5.9884e-01 -3.0503e-01 -1.0708e-01\n",
      "  4.5670e-01  7.2182e-01 -4.4221e-02 -6.8201e-02 -2.9903e-01  2.6483e-01\n",
      "  6.6810e-03  1.4919e-01 -5.3155e-01 -6.1697e-01 -5.4271e-01 -4.1103e-01\n",
      " -1.7534e-01  1.3229e-01  2.6958e-02 -1.5111e-01 -1.6844e-01 -3.0684e-01\n",
      " -5.1189e-02  9.2940e-01 -6.0882e-01 -1.1373e-01 -4.2237e-01 -4.0481e-01\n",
      " -3.4182e-01  3.8472e-01 -1.1566e-01  2.2679e-02  4.4793e-01  1.8849e-01\n",
      " -1.9598e-01 -6.8914e-02 -4.4520e-01  4.1643e-01  5.8319e-02 -3.9575e-01\n",
      " -4.0471e-01  4.2855e-01  1.6339e-01 -1.6965e-01 -5.7793e-02 -1.0358e-01\n",
      "  1.0223e+00  9.0564e-01 -5.5851e-01 -2.4856e-01  7.5582e-02 -2.7374e-01\n",
      "  3.8838e-01  5.1561e-01  6.0029e-02 -5.1990e-02  1.0113e+00  8.3607e-03\n",
      "  4.9764e-01  3.2839e-02  2.5892e-02  4.0404e-01 -3.7393e-01  1.3384e-01\n",
      " -2.9458e-01  3.4668e-01  5.2114e-02  1.0785e-01 -8.4700e-01  4.0936e-01\n",
      "  3.8801e-01  4.5079e-01 -4.3880e-01  1.9777e-01  1.9102e-01 -7.6580e-02\n",
      "  2.5788e-01  3.5642e-01  2.3305e-01 -1.6206e-01 -5.0847e-02  1.3708e-01\n",
      "  5.1338e-02 -3.8552e-01 -2.5497e-01  5.3901e-02 -6.2922e-01  3.2780e-01\n",
      " -2.0383e-01 -1.7265e-01 -9.9668e-02  2.4485e-01  3.2700e-02  3.2325e-01\n",
      "  2.3912e-01  7.7014e-03  1.2890e+00  6.8030e-02  2.0671e-02  2.6268e-01\n",
      "  1.9434e-01  5.6238e-01  7.1863e-02 -6.5502e-02 -2.2858e-01  9.2883e-02\n",
      "  3.9936e-01 -2.8038e-01  3.5633e-01  1.5203e-01 -2.7883e-01  3.8225e-02\n",
      " -3.9636e-01  3.2488e-01  2.4332e-01  2.2621e-01  2.3518e-01 -6.4747e-02\n",
      "  6.2858e-01 -1.1848e-01  1.2710e-01 -1.2697e-01  1.0015e+00 -2.5693e-01\n",
      "  2.0450e-01 -1.8789e-01 -1.2913e-01 -9.6456e-02  1.2301e-01 -1.3377e-01\n",
      " -1.2808e-01  4.1857e-02  8.6418e-01  3.6137e-01  9.0372e-02 -1.0687e-02\n",
      "  4.4736e-01  7.8975e-02 -3.2992e-01 -1.4470e-01 -6.1296e-01 -4.1543e-01\n",
      " -2.6104e-01 -8.4169e-01  4.7352e-01 -4.7640e-01 -3.4393e-01  8.2156e-02\n",
      " -2.4477e-01  7.9361e-01  2.6183e-01  4.6648e-01  1.9135e-01 -5.5033e-02\n",
      "  2.3492e-01  1.1178e-01 -3.7291e-01  2.6764e-01  1.1074e-01  6.8041e-01\n",
      " -1.0361e-03 -4.2537e-02  9.4006e-01 -2.8939e-01 -2.0841e-01  3.3001e-01\n",
      "  7.3318e-02 -5.1902e-01 -3.6412e-01 -4.1372e-01  2.0266e-01  4.3162e-01\n",
      " -1.0174e+00  2.5093e-01  3.8136e-01 -1.4332e-01 -2.0525e-01 -1.8724e-01\n",
      "  5.9925e-01  3.5604e-01 -9.1457e-01 -2.2401e-01 -4.2382e-02 -1.3299e-01\n",
      "  6.1671e-01  4.4886e-01 -1.1361e-01  2.0483e-01  2.5485e-01  2.7581e-01\n",
      " -8.7158e-01 -1.3156e-01  2.6117e-01  1.5815e-01 -3.2199e-01  7.1042e-01]\n",
      "jumps:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "over:  [-8.8137e-02 -2.1696e-02  2.9863e-01 -1.8325e-02 -2.3575e-01  1.1022e-01\n",
      " -1.7493e-01  9.9241e-03  2.3832e-01 -1.7643e+00  2.2489e-01  4.0552e-01\n",
      " -4.8176e-01 -6.6099e-02  1.3290e-01  4.7502e-01 -5.6438e-02  3.2902e-01\n",
      "  9.7628e-02  4.2467e-01  2.5285e-01 -1.7258e-01  7.6564e-02 -1.5678e-01\n",
      " -2.2694e-01 -2.1213e-01 -3.3460e-01  7.3842e-02 -4.4671e-01  3.3979e-01\n",
      " -2.3534e-01  1.5013e-01 -3.4718e-01  5.4379e-02 -8.8699e-01 -3.5534e-01\n",
      " -1.3166e-01 -1.8265e-02 -1.8708e-01  3.0193e-01  1.0008e-01 -1.1980e-01\n",
      " -7.4867e-01  1.5592e-01  4.2757e-01 -2.9996e-01 -1.2499e-01  1.2750e-01\n",
      " -2.1962e-01  4.5077e-01 -3.2268e-01 -2.8934e-01 -1.3745e-01 -1.7738e-01\n",
      "  2.4185e-01 -1.2240e-02  1.7264e-01  2.7287e-01 -8.2743e-02  2.0570e-01\n",
      "  2.2559e-02 -7.5793e-02  2.1027e-01 -3.1239e-01  1.3032e-01 -7.5693e-01\n",
      "  9.0872e-02  4.9975e-01  1.1746e-01 -5.3133e-01 -8.5348e-02  4.4042e-01\n",
      "  1.2231e-01 -1.8672e-01  1.1668e-01  1.0322e-02 -2.6779e-01  1.5042e-01\n",
      " -5.4751e-01 -3.8439e-01 -6.7117e-01 -3.4912e-01  1.4190e-01  1.2742e-01\n",
      " -1.2879e-01 -1.3582e-01  2.9059e-01  1.4339e-01  2.8467e-01  7.0642e-02\n",
      "  4.7118e-03  1.8321e-01 -4.0973e-01  2.1973e-01 -6.6693e-01 -1.0599e-01\n",
      " -3.4465e-01  2.3103e-01 -3.6278e-01 -1.2365e-02  1.1544e-01  3.6134e-02\n",
      "  7.4889e-02 -6.4933e-01 -3.3362e-02 -3.0498e-01  4.0964e-01  2.6898e-01\n",
      " -2.1150e-01 -1.1446e-01 -4.8326e-02 -5.9265e-01 -3.7301e-01 -5.4902e-02\n",
      " -1.9869e-01  2.3181e-01 -2.9017e-01  6.2177e-01  8.9856e-02 -6.4116e-01\n",
      "  3.9304e-02  9.7170e-02 -2.9954e-01  4.0426e-01  8.4010e-02  1.2902e-01\n",
      " -3.5864e-01  3.9975e-01  1.3183e-01 -1.4797e-01 -3.1449e-01  4.8654e-01\n",
      "  2.9974e-02  5.3637e-01  1.1619e-01  1.4643e-01 -2.2927e-01  7.9999e-02\n",
      "  1.3114e-01 -3.0946e-02  1.6410e-01 -3.5597e-01  2.0801e-01  3.3193e-01\n",
      " -8.4092e-01  3.3762e-01 -7.5518e-02 -4.3158e-01  1.9592e-01 -2.7815e-01\n",
      "  6.6744e-01  3.1189e-02 -1.1706e-02  5.3737e-01  2.9596e-01  3.7335e-01\n",
      " -2.2166e-01  2.3843e-02 -1.1905e-01 -1.8529e-01  1.1857e-02 -2.3540e-01\n",
      "  2.3800e-01 -3.1987e-02 -1.6400e-01 -7.9319e-02  9.9570e-02  2.6182e-02\n",
      " -6.0793e-01 -6.7515e-02  3.9870e-02  7.0721e-02 -6.0965e-01 -7.3549e-01\n",
      " -8.1241e-03 -1.3363e-01 -1.6572e-01  3.2373e-01  1.0115e-01  6.0659e-01\n",
      " -2.2547e-02  2.5960e-01  2.4251e-01 -7.8426e-02 -7.8699e-02 -4.1680e-01\n",
      " -1.4208e-01  2.5136e-02 -3.4187e-01  2.7631e-01  2.4892e-01  2.7898e-01\n",
      " -2.6522e-01 -3.3989e-01  1.7929e-01 -4.0393e-01  2.7421e-01  2.2639e-01\n",
      "  4.5234e-01 -2.4806e-01  9.4945e-01  1.4040e-01  2.3117e-02 -3.3321e-01\n",
      "  3.6958e-01 -1.7507e-01 -1.8646e-01 -2.4238e-01  2.0633e-02 -7.8795e-02\n",
      "  5.1631e-01  4.5651e-01  2.5398e-01  3.8711e-01  1.6812e-01 -5.8263e-01\n",
      " -9.9027e-02 -2.3445e-01  3.6928e-02  6.1330e-02  2.4186e-01 -2.0908e-01\n",
      " -3.8720e-02  1.4625e-01 -3.6779e-02  2.5993e-01  3.1897e-01 -5.5307e-02\n",
      " -7.7177e-02  3.0117e-01  2.3307e-01  4.1980e-01 -1.3339e-01 -2.0384e-02\n",
      "  7.1158e-01  8.4989e-02  3.2791e-01  4.8060e-01 -3.3761e-01  5.7392e-02\n",
      " -2.6536e-01  3.0228e-01  1.8540e-01 -8.3752e-02 -7.3776e-01 -1.4404e-03\n",
      "  3.4121e-01  2.9959e-02  4.2035e-01  7.3122e-02 -1.9430e-01 -2.2341e-01\n",
      "  5.8658e-01 -2.2756e-01  6.3029e-01 -6.8682e-02 -3.5365e-01  2.6851e-01\n",
      " -7.3124e-02  1.5702e-02  1.2022e-01 -2.4631e-01  2.8471e-01  3.8292e-01\n",
      " -3.8869e-01  2.0974e-01 -2.0512e-01 -1.8508e-01  3.2543e-01  3.7336e-03\n",
      " -1.9901e-02 -8.4242e-02  2.9326e-01 -5.8162e-02  3.6575e-01  4.6895e-02\n",
      " -2.1328e+00 -3.5109e-01  4.1716e-01  1.8393e-01 -3.0990e-01  2.7303e-01\n",
      "  1.6627e-02  4.4978e-03 -7.2902e-02  5.3814e-02 -5.7213e-02  1.5815e-01\n",
      "  1.4517e-01  1.6580e-01 -7.3557e-02  1.6125e-01 -9.9237e-02  2.6099e-01\n",
      "  2.7941e-02  3.2140e-01  1.6931e-01 -3.5469e-01 -4.2713e-01 -3.9323e-01]\n",
      "the:  [ 4.6560e-02  2.1318e-01 -7.4364e-03 -4.5854e-01 -3.5639e-02  2.3643e-01\n",
      " -2.8836e-01  2.1521e-01 -1.3486e-01 -1.6413e+00 -2.6091e-01  3.2434e-02\n",
      "  5.6621e-02 -4.3296e-02 -2.1672e-02  2.2476e-01 -7.5129e-02 -6.7018e-02\n",
      " -1.4247e-01  3.8825e-02 -1.8951e-01  2.9977e-01  3.9305e-01  1.7887e-01\n",
      " -1.7343e-01 -2.1178e-01  2.3617e-01 -6.3681e-02 -4.2318e-01 -1.1661e-01\n",
      "  9.3754e-02  1.7296e-01 -3.3073e-01  4.9112e-01 -6.8995e-01 -9.2462e-02\n",
      "  2.4742e-01 -1.7991e-01  9.7908e-02  8.3118e-02  1.5299e-01 -2.7276e-01\n",
      " -3.8934e-02  5.4453e-01  5.3737e-01  2.9105e-01 -7.3514e-03  4.7880e-02\n",
      " -4.0760e-01 -2.6759e-02  1.7919e-01  1.0977e-02 -1.0963e-01 -2.6395e-01\n",
      "  7.3990e-02  2.6236e-01 -1.5080e-01  3.4623e-01  2.5758e-01  1.1971e-01\n",
      " -3.7135e-02 -7.1593e-02  4.3898e-01 -4.0764e-02  1.6425e-02 -4.4640e-01\n",
      "  1.7197e-01  4.6246e-02  5.8639e-02  4.1499e-02  5.3948e-01  5.2495e-01\n",
      "  1.1361e-01 -4.8315e-02 -3.6385e-01  1.8704e-01  9.2761e-02 -1.1129e-01\n",
      " -4.2085e-01  1.3992e-01 -3.9338e-01 -6.7945e-02  1.2188e-01  1.6707e-01\n",
      "  7.5169e-02 -1.5529e-02 -1.9499e-01  1.9638e-01  5.3194e-02  2.5170e-01\n",
      " -3.4845e-01 -1.0638e-01 -3.4692e-01 -1.9024e-01 -2.0040e-01  1.2154e-01\n",
      " -2.9208e-01  2.3353e-02 -1.1618e-01 -3.5768e-01  6.2304e-02  3.5884e-01\n",
      "  2.9060e-02  7.3005e-03  4.9482e-03 -1.5048e-01 -1.2313e-01  1.9337e-01\n",
      "  1.2173e-01  4.4503e-01  2.5147e-01  1.0781e-01 -1.7716e-01  3.8691e-02\n",
      "  8.1530e-02  1.4667e-01  6.3666e-02  6.1332e-02 -7.5569e-02 -3.7724e-01\n",
      "  1.5850e-02 -3.0342e-01  2.8374e-01 -4.2013e-02 -4.0715e-02 -1.5269e-01\n",
      "  7.4980e-02  1.5577e-01  1.0433e-01  3.1393e-01  1.9309e-01  1.9429e-01\n",
      "  1.5185e-01 -1.0192e-01 -1.8785e-02  2.0791e-01  1.3366e-01  1.9038e-01\n",
      " -2.5558e-01  3.0400e-01 -1.8960e-02  2.0147e-01 -4.2110e-01 -7.5156e-03\n",
      " -2.7977e-01 -1.9314e-01  4.6204e-02  1.9971e-01 -3.0207e-01  2.5735e-01\n",
      "  6.8107e-01 -1.9409e-01  2.3984e-01  2.2493e-01  6.5224e-01 -1.3561e-01\n",
      " -1.7383e-01 -4.8209e-02 -1.1860e-01  2.1588e-03 -1.9525e-02  1.1948e-01\n",
      "  1.9346e-01 -4.0820e-01 -8.2966e-02  1.6626e-01 -1.0601e-01  3.5861e-01\n",
      "  1.6922e-01  7.2590e-02 -2.4803e-01 -1.0024e-01 -5.2491e-01 -1.7745e-01\n",
      " -3.6647e-01  2.6180e-01 -1.2077e-02  8.3190e-02 -2.1528e-01  4.1045e-01\n",
      "  2.9136e-01  3.0869e-01  7.8864e-02  3.2207e-01 -4.1023e-02 -1.0970e-01\n",
      " -9.2041e-02 -1.2339e-01 -1.6416e-01  3.5382e-01 -8.2774e-02  3.3171e-01\n",
      " -2.4738e-01 -4.8928e-02  1.5746e-01  1.8988e-01 -2.6642e-02  6.3315e-02\n",
      " -1.0673e-02  3.4089e-01  1.4106e+00  1.3417e-01  2.8191e-01 -2.5940e-01\n",
      "  5.5267e-02 -5.2425e-02 -2.5789e-01  1.9127e-02 -2.2084e-02  3.2113e-01\n",
      "  6.8818e-02  5.1207e-01  1.6478e-01 -2.0194e-01  2.9232e-01  9.8575e-02\n",
      "  1.3145e-02 -1.0652e-01  1.3510e-01 -4.5332e-02  2.0697e-01 -4.8425e-01\n",
      " -4.4706e-01  3.3305e-03  2.9264e-03 -1.0975e-01 -2.3325e-01  2.2442e-01\n",
      " -1.0503e-01  1.2339e-01  1.0978e-01  4.8994e-02 -2.5157e-01  4.0319e-01\n",
      "  3.5318e-01  1.8651e-01 -2.3622e-02 -1.2734e-01  1.1475e-01  2.7359e-01\n",
      " -2.1866e-01  1.5794e-02  8.1754e-01 -2.3792e-02 -8.5469e-01 -1.6203e-01\n",
      "  1.8076e-01  2.8014e-02 -1.4340e-01  1.3139e-03 -9.1735e-02 -8.9704e-02\n",
      "  1.1105e-01 -1.6703e-01  6.8377e-02 -8.7388e-02 -3.9789e-02  1.4184e-02\n",
      "  2.1187e-01  2.8579e-01 -2.8797e-01 -5.8996e-02 -3.2436e-02 -4.7009e-03\n",
      " -1.7052e-01 -3.4741e-02 -1.1489e-01  7.5093e-02  9.9526e-02  4.8183e-02\n",
      " -7.3775e-02 -4.1817e-01  4.1268e-03  4.4414e-01 -1.6062e-01  1.4294e-01\n",
      " -2.2628e+00 -2.7347e-02  8.1311e-01  7.7417e-01 -2.5639e-01 -1.1576e-01\n",
      " -1.1982e-01 -2.1363e-01  2.8429e-02  2.7261e-01  3.1026e-02  9.6782e-02\n",
      "  6.7769e-03  1.4082e-01 -1.3064e-02 -2.9686e-01 -7.9913e-02  1.9500e-01\n",
      "  3.1549e-02  2.8506e-01 -8.7461e-02  9.0611e-03 -2.0989e-01  5.3913e-02]\n",
      "lazy:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "dog:  [-1.1043e-01  8.1217e-01  7.3668e-02  1.9023e-01 -5.2888e-02  6.1468e-02\n",
      "  1.6076e-01  4.1302e-01 -3.0199e-01 -9.0827e-01  2.7504e-01 -3.1890e-02\n",
      " -2.8842e-01  2.3447e-01  4.7679e-01  5.0124e-01  2.9371e-01  2.7029e-01\n",
      "  5.4745e-02  9.8038e-02  5.7116e-01  3.6755e-01  4.0734e-02  3.4347e-01\n",
      " -1.8256e-01 -2.8935e-01  2.3826e-02 -1.9401e-01  2.4444e-01  1.3407e-01\n",
      " -1.6494e-01 -2.6983e-01 -2.6234e-01 -2.1779e-01 -8.7528e-01  7.3822e-01\n",
      " -8.7931e-02 -1.0876e-02 -2.6540e-01  3.4668e-01 -5.5814e-01  1.7591e-01\n",
      "  1.6926e-01 -1.5725e-01 -5.0430e-01 -2.0100e-01  6.6701e-01 -3.2518e-02\n",
      "  4.5012e-02  6.5675e-02 -1.6061e-01 -7.3363e-01  2.4642e-01  3.4325e-01\n",
      "  2.1899e-01  4.8646e-02 -5.9987e-01 -5.8153e-02 -5.1694e-02 -5.7846e-01\n",
      "  3.0000e-01  3.5078e-01  4.6646e-01 -7.5309e-03  1.0455e-01 -5.1016e-01\n",
      " -5.5987e-02 -1.0295e-01 -2.6476e-01 -4.1230e-02 -2.8371e-02  5.1979e-01\n",
      " -3.4849e-01 -4.7217e-01 -3.7229e-01 -3.2790e-02  1.3989e-01  3.5716e-01\n",
      "  1.9305e-01 -2.1986e-01  2.4136e-01  4.0976e-01  3.7516e-01  1.4255e-01\n",
      " -3.4143e-02 -7.2653e-01 -1.0832e-01  6.8616e-01 -2.6335e-01 -4.2345e-01\n",
      " -2.4253e-01  1.5778e-01  1.4258e-01 -3.2749e-01 -3.4699e-01  1.6148e-01\n",
      "  1.9603e-01  4.1639e-01 -2.3370e-01  7.5816e-02  1.5899e-01  1.6623e-03\n",
      " -4.8301e-02 -1.0611e-01 -1.9326e-01  1.4494e-01  1.5406e-02  1.0629e-01\n",
      " -3.6699e-02  6.3230e-01  1.2986e-01  4.9902e-01 -1.1323e+00 -1.2636e-01\n",
      "  6.4718e-02  1.2374e-01 -4.9712e-01 -1.4836e-02  1.0488e-01 -4.9818e-01\n",
      " -2.8856e-01  3.8949e-01 -3.1828e-02 -2.8625e-01 -9.8758e-02 -7.6990e-02\n",
      " -2.4234e-01  7.5793e-01  3.4835e-01 -7.1030e-01  4.5318e-01 -3.4418e-01\n",
      " -1.9459e-01  6.1478e-01 -2.9010e-02 -2.7864e-01  3.8556e-01  1.0072e-01\n",
      "  1.2895e-01  1.7992e-02  3.3670e-01  2.0698e-01 -3.8049e-01 -6.6661e-03\n",
      "  1.1540e-01 -8.5268e-02 -1.4608e-01  4.4514e-01 -9.3674e-02  2.3639e-01\n",
      " -1.1447e-01  1.0948e+00 -5.7823e-02 -1.6295e-01  5.5880e-01 -1.8988e-02\n",
      " -7.1374e-02  2.1319e-01  6.1277e-02  7.2759e-01  6.2747e-01 -1.9280e-01\n",
      "  1.3057e-01  1.7426e-01 -1.0229e-01  1.5232e-01  5.2500e-01 -2.1919e-01\n",
      " -2.7185e-01 -5.4186e-01  3.1752e-01  1.6375e-01 -2.9039e-01  1.7074e-01\n",
      " -3.1814e-01 -9.6421e-01 -1.1610e-01 -2.9951e-01  1.8686e-01 -4.5986e-01\n",
      "  4.1633e-01 -1.7583e-01 -3.4583e-01 -2.7244e-01 -5.0216e-01  1.2852e-02\n",
      "  5.9838e-01 -1.1237e-01  2.4697e-01 -4.9048e-01 -4.4188e-01 -1.6255e-01\n",
      " -7.3313e-01 -3.7677e-01 -6.8925e-01  6.1174e-02 -4.2101e-01 -1.3153e-01\n",
      " -8.3590e-03 -1.8360e-02  1.3686e+00  4.6169e-02  9.4622e-01 -1.5126e-02\n",
      " -1.2477e-01  4.8754e-01  2.2384e-01 -2.1820e-01 -2.3389e-01  1.5207e-01\n",
      " -2.8718e-01 -6.3908e-01 -2.2383e-01 -1.8014e-01 -3.3548e-01  5.3587e-01\n",
      " -2.9367e-01  1.0866e-01  6.3411e-02 -9.3424e-03 -1.5886e-01  2.2602e-01\n",
      "  1.1925e-01 -4.1442e-01 -7.8062e-02 -9.7857e-02  2.7938e-01 -1.8348e-01\n",
      " -3.4584e-01  1.8489e-01  1.7402e-01 -5.2198e-01 -4.3306e-01  1.6256e-01\n",
      "  1.4032e-01  3.5124e-01 -1.8280e-01 -3.5984e-01 -1.3009e-01  1.6304e-01\n",
      "  3.1734e-01  3.7716e-03 -4.5498e-02 -4.2066e-01 -4.4419e-01 -6.8985e-01\n",
      " -4.9359e-01  7.0281e-02 -1.4377e-01  6.2508e-01 -5.6311e-02  1.8850e-01\n",
      " -5.6785e-02  1.4052e-01  1.1973e+00  7.1894e-01  5.4332e-01 -1.2461e-01\n",
      " -1.1978e-01  3.0163e-01 -1.6273e-01 -4.6740e-02 -2.5249e-01 -3.0659e-02\n",
      " -3.2271e-01  3.2361e-01  3.3244e-01 -2.7819e-02 -3.3367e-01 -2.3444e-02\n",
      " -5.0394e-01 -2.0587e-01 -1.3013e-01 -3.5884e-01  4.5384e-02 -1.1863e-01\n",
      " -1.7257e+00  3.9441e-01 -5.3179e-01  5.8209e-01 -6.5771e-01  3.6849e-01\n",
      "  2.3518e-01  1.0802e-01 -8.3159e-01  6.1486e-01  2.5547e-01 -4.5289e-01\n",
      "  5.1446e-01 -1.7911e-01 -1.2389e-01  1.8688e-01 -4.1102e-01 -7.0877e-01\n",
      " -3.7501e-01 -6.6152e-01  6.7730e-01  3.3936e-01  5.7994e-01  6.8149e-02]\n"
     ]
    }
   ],
   "source": [
    "# Verify the new vocabulary: should get same embeddings for test sentence\n",
    "# Note that a small EVOCABSIZE may yield some zero vectors for embeddings\n",
    "print('\\nTest sentence embeddings from vocabulary of', EVOCABSIZE, 'words:\\n')\n",
    "for word in words_in_test_sentence:\n",
    "    word_ = word.lower()\n",
    "    embedding = limited_index_to_embedding[limited_word_to_index[word_]]\n",
    "    print(word_ + \": \", embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of lists of lists for embeddings\n",
    "embeddings = []    \n",
    "for doc in documents:\n",
    "    embedding = []\n",
    "    for word in doc:\n",
    "       embedding.append(limited_index_to_embedding[limited_word_to_index[word]]) \n",
    "    embeddings.append(embedding)\n",
    "\n",
    "# -----------------------------------------------------    \n",
    "# Make embeddings a numpy array for use in an RNN \n",
    "# Create training and test sets with Scikit Learn\n",
    "# -----------------------------------------------------\n",
    "embeddings_array = np.array(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random splitting of the data in to training (80%) and test (20%)  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings_array, thumbs_down_up, test_size=0.20, random_state = RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ---- Epoch  0  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.53 Test accuracy: 0.53\n",
      "\n",
      "  ---- Epoch  1  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.58 Test accuracy: 0.565\n",
      "\n",
      "  ---- Epoch  2  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.63 Test accuracy: 0.6\n",
      "\n",
      "  ---- Epoch  3  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.67 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  4  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.68 Test accuracy: 0.585\n",
      "\n",
      "  ---- Epoch  5  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.76 Test accuracy: 0.595\n",
      "\n",
      "  ---- Epoch  6  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.74 Test accuracy: 0.61\n",
      "\n",
      "  ---- Epoch  7  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.73 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  8  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.75 Test accuracy: 0.6\n",
      "\n",
      "  ---- Epoch  9  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.76 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  10  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.77 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  11  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.79 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  12  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.8 Test accuracy: 0.645\n",
      "\n",
      "  ---- Epoch  13  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.83 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  14  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.84 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  15  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.85 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  16  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.84 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  17  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Train accuracy: 0.86 Test accuracy: 0.68\n",
      "\n",
      "  ---- Epoch  18  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.89 Test accuracy: 0.68\n",
      "\n",
      "  ---- Epoch  19  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.9 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  20  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.89 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  21  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.89 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  22  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.91 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  23  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.91 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  24  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.93 Test accuracy: 0.68\n",
      "\n",
      "  ---- Epoch  25  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.93 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  26  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.92 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  27  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.89 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  28  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.91 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  29  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.95 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  30  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.94 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  31  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.95 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  32  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.95 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  33  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.95 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  34  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.95 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  35  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.95 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  36  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.95 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  37  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.98 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  38  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.97 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  39  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.96 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  40  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.96 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  41  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.96 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  42  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.98 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  43  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.97 Test accuracy: 0.63\n",
      "\n",
      "  ---- Epoch  44  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  45  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  46  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.95 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  47  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.96 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  48  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.625\n",
      "\n",
      "  ---- Epoch  49  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.98 Test accuracy: 0.625\n",
      "Duration in minutes: 0.31\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_steps = embeddings_array.shape[1]  # number of words per document \n",
    "n_inputs = embeddings_array.shape[2]  # dimension of  pre-trained embeddings\n",
    "n_neurons = 20  # analyst specified number of neurons\n",
    "n_outputs = 2  # thumbs-down or thumbs-up\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Start timer\n",
    "start = time.clock()\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "\n",
    "logits = tf.layers.dense(states, n_outputs)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                          logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        print('\\n  ---- Epoch ', epoch, ' ----\\n')\n",
    "        for iteration in range(y_train.shape[0] // batch_size):          \n",
    "            X_batch = X_train[iteration*batch_size:(iteration + 1)*batch_size,:]\n",
    "            y_batch = y_train[iteration*batch_size:(iteration + 1)*batch_size]\n",
    "            print('  Batch ', iteration, ' training observations from ',  \n",
    "                  iteration*batch_size, ' to ', (iteration + 1)*batch_size-1,)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        print('\\n  Train accuracy:', acc_train, 'Test accuracy:', acc_test)\n",
    "        \n",
    "# Record the clock time it takes\n",
    "duration = round((time.clock() - start ) / 60, 2)\n",
    "print('Duration in minutes:', duration)\n",
    "\n",
    "#acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "\n",
    "metrics['Model 3'] = ['glove.6B.300d', n_epochs, EVOCABSIZE, duration, acc_train, acc_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4\n",
    "\n",
    "After defining the vocabulary size for the test (30.000), the vocabulary size is reduced to the size of the n most frequently used words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30001, 300)\n"
     ]
    }
   ],
   "source": [
    "EVOCABSIZE = 30000  # specify desired size of pre-defined embedding vocabulary \n",
    "\n",
    "vocab_size, embedding_dim = index_to_embedding.shape\n",
    "\n",
    "# dictionary has the items() function, returns list of (key, value) tuples\n",
    "limited_word_to_index = defaultdict(default_factory, \\\n",
    "    {k: v for k, v in word_to_index.items() if v < EVOCABSIZE})\n",
    "\n",
    "\n",
    "# Select the first EVOCABSIZE rows to the index_to_embedding\n",
    "limited_index_to_embedding = index_to_embedding[0:EVOCABSIZE,:]\n",
    "\n",
    "# Set the unknown-word row to be all zeros as previously\n",
    "limited_index_to_embedding = np.append(limited_index_to_embedding, \n",
    "    index_to_embedding[index_to_embedding.shape[0] - 1, :].\\\n",
    "        reshape(1,embedding_dim), \n",
    "    axis = 0)\n",
    "\n",
    "print(limited_index_to_embedding.shape)\n",
    "\n",
    "del index_to_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test sentence embeddings from vocabulary of 30000 words:\n",
      "\n",
      "the:  [ 4.6560e-02  2.1318e-01 -7.4364e-03 -4.5854e-01 -3.5639e-02  2.3643e-01\n",
      " -2.8836e-01  2.1521e-01 -1.3486e-01 -1.6413e+00 -2.6091e-01  3.2434e-02\n",
      "  5.6621e-02 -4.3296e-02 -2.1672e-02  2.2476e-01 -7.5129e-02 -6.7018e-02\n",
      " -1.4247e-01  3.8825e-02 -1.8951e-01  2.9977e-01  3.9305e-01  1.7887e-01\n",
      " -1.7343e-01 -2.1178e-01  2.3617e-01 -6.3681e-02 -4.2318e-01 -1.1661e-01\n",
      "  9.3754e-02  1.7296e-01 -3.3073e-01  4.9112e-01 -6.8995e-01 -9.2462e-02\n",
      "  2.4742e-01 -1.7991e-01  9.7908e-02  8.3118e-02  1.5299e-01 -2.7276e-01\n",
      " -3.8934e-02  5.4453e-01  5.3737e-01  2.9105e-01 -7.3514e-03  4.7880e-02\n",
      " -4.0760e-01 -2.6759e-02  1.7919e-01  1.0977e-02 -1.0963e-01 -2.6395e-01\n",
      "  7.3990e-02  2.6236e-01 -1.5080e-01  3.4623e-01  2.5758e-01  1.1971e-01\n",
      " -3.7135e-02 -7.1593e-02  4.3898e-01 -4.0764e-02  1.6425e-02 -4.4640e-01\n",
      "  1.7197e-01  4.6246e-02  5.8639e-02  4.1499e-02  5.3948e-01  5.2495e-01\n",
      "  1.1361e-01 -4.8315e-02 -3.6385e-01  1.8704e-01  9.2761e-02 -1.1129e-01\n",
      " -4.2085e-01  1.3992e-01 -3.9338e-01 -6.7945e-02  1.2188e-01  1.6707e-01\n",
      "  7.5169e-02 -1.5529e-02 -1.9499e-01  1.9638e-01  5.3194e-02  2.5170e-01\n",
      " -3.4845e-01 -1.0638e-01 -3.4692e-01 -1.9024e-01 -2.0040e-01  1.2154e-01\n",
      " -2.9208e-01  2.3353e-02 -1.1618e-01 -3.5768e-01  6.2304e-02  3.5884e-01\n",
      "  2.9060e-02  7.3005e-03  4.9482e-03 -1.5048e-01 -1.2313e-01  1.9337e-01\n",
      "  1.2173e-01  4.4503e-01  2.5147e-01  1.0781e-01 -1.7716e-01  3.8691e-02\n",
      "  8.1530e-02  1.4667e-01  6.3666e-02  6.1332e-02 -7.5569e-02 -3.7724e-01\n",
      "  1.5850e-02 -3.0342e-01  2.8374e-01 -4.2013e-02 -4.0715e-02 -1.5269e-01\n",
      "  7.4980e-02  1.5577e-01  1.0433e-01  3.1393e-01  1.9309e-01  1.9429e-01\n",
      "  1.5185e-01 -1.0192e-01 -1.8785e-02  2.0791e-01  1.3366e-01  1.9038e-01\n",
      " -2.5558e-01  3.0400e-01 -1.8960e-02  2.0147e-01 -4.2110e-01 -7.5156e-03\n",
      " -2.7977e-01 -1.9314e-01  4.6204e-02  1.9971e-01 -3.0207e-01  2.5735e-01\n",
      "  6.8107e-01 -1.9409e-01  2.3984e-01  2.2493e-01  6.5224e-01 -1.3561e-01\n",
      " -1.7383e-01 -4.8209e-02 -1.1860e-01  2.1588e-03 -1.9525e-02  1.1948e-01\n",
      "  1.9346e-01 -4.0820e-01 -8.2966e-02  1.6626e-01 -1.0601e-01  3.5861e-01\n",
      "  1.6922e-01  7.2590e-02 -2.4803e-01 -1.0024e-01 -5.2491e-01 -1.7745e-01\n",
      " -3.6647e-01  2.6180e-01 -1.2077e-02  8.3190e-02 -2.1528e-01  4.1045e-01\n",
      "  2.9136e-01  3.0869e-01  7.8864e-02  3.2207e-01 -4.1023e-02 -1.0970e-01\n",
      " -9.2041e-02 -1.2339e-01 -1.6416e-01  3.5382e-01 -8.2774e-02  3.3171e-01\n",
      " -2.4738e-01 -4.8928e-02  1.5746e-01  1.8988e-01 -2.6642e-02  6.3315e-02\n",
      " -1.0673e-02  3.4089e-01  1.4106e+00  1.3417e-01  2.8191e-01 -2.5940e-01\n",
      "  5.5267e-02 -5.2425e-02 -2.5789e-01  1.9127e-02 -2.2084e-02  3.2113e-01\n",
      "  6.8818e-02  5.1207e-01  1.6478e-01 -2.0194e-01  2.9232e-01  9.8575e-02\n",
      "  1.3145e-02 -1.0652e-01  1.3510e-01 -4.5332e-02  2.0697e-01 -4.8425e-01\n",
      " -4.4706e-01  3.3305e-03  2.9264e-03 -1.0975e-01 -2.3325e-01  2.2442e-01\n",
      " -1.0503e-01  1.2339e-01  1.0978e-01  4.8994e-02 -2.5157e-01  4.0319e-01\n",
      "  3.5318e-01  1.8651e-01 -2.3622e-02 -1.2734e-01  1.1475e-01  2.7359e-01\n",
      " -2.1866e-01  1.5794e-02  8.1754e-01 -2.3792e-02 -8.5469e-01 -1.6203e-01\n",
      "  1.8076e-01  2.8014e-02 -1.4340e-01  1.3139e-03 -9.1735e-02 -8.9704e-02\n",
      "  1.1105e-01 -1.6703e-01  6.8377e-02 -8.7388e-02 -3.9789e-02  1.4184e-02\n",
      "  2.1187e-01  2.8579e-01 -2.8797e-01 -5.8996e-02 -3.2436e-02 -4.7009e-03\n",
      " -1.7052e-01 -3.4741e-02 -1.1489e-01  7.5093e-02  9.9526e-02  4.8183e-02\n",
      " -7.3775e-02 -4.1817e-01  4.1268e-03  4.4414e-01 -1.6062e-01  1.4294e-01\n",
      " -2.2628e+00 -2.7347e-02  8.1311e-01  7.7417e-01 -2.5639e-01 -1.1576e-01\n",
      " -1.1982e-01 -2.1363e-01  2.8429e-02  2.7261e-01  3.1026e-02  9.6782e-02\n",
      "  6.7769e-03  1.4082e-01 -1.3064e-02 -2.9686e-01 -7.9913e-02  1.9500e-01\n",
      "  3.1549e-02  2.8506e-01 -8.7461e-02  9.0611e-03 -2.0989e-01  5.3913e-02]\n",
      "quick:  [ 0.37594    0.063183   0.51835   -0.48652    0.34026   -0.17801\n",
      "  0.32615   -0.32989    0.25508   -1.026     -0.13841    0.27258\n",
      " -0.010244   0.35186    0.28341    0.3189    -0.18892   -0.292\n",
      " -0.071297   0.25631   -0.34286    0.16179    0.065725  -0.038052\n",
      "  0.1457     0.20289    0.14274   -0.15024    0.35412   -0.13715\n",
      " -0.2677    -0.011243  -0.1541     0.1765    -1.5424     0.37699\n",
      " -0.28239    0.19172   -0.46349   -0.26345   -0.39337    0.41276\n",
      "  0.42873   -0.1317     0.31096   -0.28715    0.47212   -0.41866\n",
      " -0.11871   -0.026621  -0.21089   -0.074757   0.26429   -0.032246\n",
      " -0.084233   0.48653   -0.38692   -0.3756    -0.27031    0.15208\n",
      "  0.2263    -0.28777    0.12475   -0.17029    0.019322   0.34175\n",
      "  0.69492   -0.42039    0.3161    -0.23871    0.30485   -0.078005\n",
      "  0.25937   -0.036324   0.27365   -0.1698    -0.10004    0.13614\n",
      "  0.0094279 -0.51261    0.3464     0.032112   0.25292    0.1947\n",
      "  0.16906   -0.044683  -0.039252   0.31651   -0.27185    0.10862\n",
      "  0.070371   0.31916   -0.55993   -0.5553     0.5493    -0.17244\n",
      " -0.70848    0.039063   0.33553   -0.11393   -0.28882   -0.53623\n",
      "  0.0021584  0.24971    0.31383   -0.2516    -0.28619    0.20113\n",
      " -0.29545   -0.3285     0.33289    0.19422    0.047601  -0.13157\n",
      "  0.4269     0.085041  -0.30294   -0.38344   -0.035083  -0.0463\n",
      "  0.035454  -0.052446   0.51216   -0.37809   -0.24834    0.28464\n",
      "  0.019408   0.61137    0.14859    0.30104    0.23773    0.37627\n",
      " -0.64467    0.19701   -0.19264   -0.013601   0.073281  -0.43031\n",
      "  0.38081   -0.42172   -0.16131    0.12108   -0.12078   -0.20818\n",
      " -0.4697     0.1279    -0.63088    0.16412    0.20474    0.16701\n",
      " -0.79632   -0.075741  -0.25251   -0.025189   0.081245  -0.081758\n",
      " -0.12925   -0.33034    0.039839  -0.30436    0.023003  -0.35589\n",
      "  0.40923   -0.10969   -0.084268   0.56261    0.37604    0.10676\n",
      " -0.1678     0.11219   -0.13141   -0.025916  -0.56102   -0.074779\n",
      " -0.14769    0.13028   -0.38351    0.055938   0.1996     0.0052525\n",
      "  0.11655   -0.58141    0.45407   -0.11067   -0.10262    0.31478\n",
      " -0.049739  -0.34926   -0.016468  -0.12476   -0.071381   0.34803\n",
      " -0.12247   -0.38406    0.095986   0.12451   -0.033609  -0.62353\n",
      "  0.25048   -0.1427     0.60613   -0.080829   0.25008    0.059055\n",
      " -0.17486    0.14913   -0.41488    0.27573   -0.11921   -0.02267\n",
      " -0.34188   -0.49563   -0.22119    0.49553   -0.035482  -0.11908\n",
      "  0.0096008 -0.44059   -0.35947   -0.19156    0.28505    0.35236\n",
      " -0.3384    -0.28643   -0.22068   -0.29761    0.10412   -0.067384\n",
      "  0.043089  -0.05794   -0.31212    0.24026    0.072559  -0.024896\n",
      " -0.19299    0.020044  -0.10826    0.2022     0.097076   0.43886\n",
      "  0.35085    0.38611   -0.1838    -0.047166  -0.5351    -0.17215\n",
      "  0.17407    0.17959   -0.35965   -0.23817    0.16348   -0.79488\n",
      "  0.18858    0.027404   0.23823    0.047581   0.2485     0.18332\n",
      " -0.22626    0.54554   -0.31324   -0.22699   -0.31341    0.68296\n",
      "  0.13422   -0.27644   -0.38901   -0.29207   -0.10058    0.12057\n",
      " -0.36691   -0.69507   -0.22242   -0.023121   0.72283    0.0051197\n",
      " -1.7769     0.40651    0.025966  -0.18157    0.23957    0.37943\n",
      "  0.67713    0.49789    0.3634    -0.60131    0.53868   -0.18682\n",
      " -0.14783    0.40581    0.1379     0.054337  -0.12388    0.064828\n",
      "  0.27453    0.5165    -0.1955    -0.55939   -0.2744    -0.12146  ]\n",
      "brown:  [ 0.2793     0.18372   -0.11257    0.21734   -0.21657   -0.50335\n",
      " -0.27194    0.32181    0.031892  -0.37998    0.15544   -0.32953\n",
      " -0.19827    0.20403    0.26768    0.292     -0.34187   -0.10766\n",
      " -0.43697   -0.14488    0.14634    0.21591    0.12576    0.14895\n",
      " -0.21763    0.030797   0.10949   -0.41689   -0.30296   -0.14592\n",
      " -0.56228    0.33282   -0.20436   -0.24403   -1.4732     0.68345\n",
      "  0.45336    0.43671   -0.15641    0.15075   -0.24265   -0.040059\n",
      "  0.22323    0.19523    0.37445   -0.18509   -0.10302   -0.055363\n",
      " -0.17274   -0.45401   -0.14729   -0.24133   -0.043826  -0.23243\n",
      "  0.42367    0.15906   -0.14039   -0.36185   -0.26695   -0.42724\n",
      " -0.08843   -0.099597   0.24257   -0.05424    0.10746   -1.1304\n",
      "  0.024651  -0.10212    0.046319  -0.68792    0.4214    -0.25844\n",
      "  0.17052    0.097878   0.026835   0.32044    0.0062988  0.24575\n",
      "  0.20126   -0.16771    0.19825    0.28939   -0.064994  -0.38766\n",
      "  0.52509    0.38195    0.32421    0.20683   -0.48472   -0.080334\n",
      " -0.15345    0.35459   -0.43765    0.071575  -0.39516   -0.22906\n",
      "  0.25686    0.26659    0.37626   -0.18556    0.16445   -0.33614\n",
      " -0.56262    0.067852  -0.61642    0.19546    0.45027    0.20238\n",
      "  0.33957    0.41372    0.11855    0.087619   0.18754    0.17901\n",
      "  0.022569  -0.10854   -0.47226    0.41039    0.32588   -0.58468\n",
      " -0.0057296 -0.29201   -0.12777   -0.15729   -0.40103   -0.039414\n",
      " -0.1192     0.40093    0.032862   0.39862   -0.63525    0.11594\n",
      " -0.39954    0.36919   -0.50021   -0.51169   -0.13955    0.18055\n",
      " -0.079918  -0.19474    0.53131    0.093723   0.2773    -0.40505\n",
      " -0.20568    0.11139    0.032661  -0.04852    0.44576    0.23667\n",
      "  0.54981    0.23585   -0.51539   -0.46424    0.021099  -0.3919\n",
      "  0.58338   -0.89908    0.094066   0.30159   -0.063199  -0.31635\n",
      "  0.50333   -0.068517  -0.38681    0.33      -0.49463    0.75491\n",
      " -0.088266  -0.19413    0.4238    -0.031727  -0.4464    -0.21028\n",
      " -0.11151   -0.07088   -0.027832  -0.63304    0.27336   -0.47925\n",
      " -0.03239    0.46069    0.16968   -0.38262   -0.31413   -0.29068\n",
      " -0.031801  -0.48974   -0.50999    0.1466     0.0027995  0.56333\n",
      " -0.044347  -0.085679   0.20559   -0.051593   0.75228   -0.013291\n",
      " -0.084694  -0.4305     1.1734    -0.083233   0.1561    -0.15758\n",
      "  0.19066   -0.2966     0.63704    0.45616   -0.34797   -0.12732\n",
      "  0.4901    -0.51217   -0.063474  -0.061496   0.28825    0.17711\n",
      "  0.46301   -0.12697   -0.044627  -1.0064     0.76394    0.20494\n",
      "  0.028766   0.27597    0.021726  -0.12054    0.23284    0.18999\n",
      "  0.30048   -0.056139   0.09546   -0.036514   0.0084885  0.016599\n",
      " -0.31428   -0.2707     0.099281   0.4445    -0.36      -0.55556\n",
      " -0.18551   -0.30644    0.056475  -0.19197   -0.48886    0.33044\n",
      "  0.19535   -0.53828    0.12385   -0.29372   -0.1036     0.0051129\n",
      "  0.11483   -0.10591    0.73337    0.26978   -0.06925    0.11565\n",
      "  0.27711    0.15109   -0.069137  -0.14481    0.32319    0.039345\n",
      " -0.44964    0.27103    0.045326  -0.064534  -0.37144    0.47615\n",
      " -0.61105   -0.11922   -0.068806   0.15401   -0.40812    0.32575\n",
      " -1.2888     0.0203    -0.12893   -0.22211   -0.16402    0.29018\n",
      "  0.36295   -0.081025  -0.50492    0.5046    -0.37485    0.52111\n",
      "  0.1757     0.069686   0.48937   -0.17747   -0.20577    0.70419\n",
      "  0.068633   0.47878   -0.21754   -0.016868  -0.91378    0.45643  ]\n",
      "fox:  [-1.1570e-01 -2.5048e-02 -1.1013e-01 -4.8060e-02 -8.5504e-02  3.7308e-01\n",
      " -9.4790e-01  5.9662e-01 -2.0006e-02 -3.0469e-01  2.4997e-01 -2.0005e-01\n",
      "  1.9284e-01  4.7197e-01  2.7099e-01  3.4190e-01 -2.9186e-01 -3.9718e-01\n",
      "  5.5653e-01  3.2774e-01  9.4860e-02 -4.6588e-01  6.2119e-01  2.0709e-01\n",
      "  8.2332e-02 -1.0175e-01  3.2067e-01 -3.1875e-01  1.7941e-01 -2.4127e-01\n",
      " -1.7961e-02 -8.7154e-02  3.3423e-01 -4.5026e-02 -1.2193e+00  1.9321e-01\n",
      "  2.1427e-01  2.3893e-01 -1.8084e-01 -1.5920e-01 -1.0387e-01  2.2060e-01\n",
      "  2.5423e-01  2.0815e-01 -3.3072e-01 -2.0672e-01 -3.1424e-01  2.6498e-02\n",
      "  2.1955e-01 -3.1157e-01 -7.8914e-02 -1.5089e-02 -4.0848e-02 -2.3807e-01\n",
      "  3.1983e-01 -7.4173e-02  2.2983e-01  5.5735e-02  5.6743e-03 -7.7368e-01\n",
      "  2.7412e-01 -1.8999e-01  5.3825e-01 -1.9784e-01 -1.0898e-01 -3.4022e-01\n",
      "  3.1582e-01  5.0317e-01  3.3955e-01 -2.7143e-01 -3.3026e-02  3.4279e-01\n",
      " -4.9815e-01  2.5056e-01 -1.2791e-01 -1.5912e-01 -8.7541e-03  1.0450e-01\n",
      "  2.1969e-01 -5.9077e-02 -3.7600e-01  6.0835e-02  1.8427e-01 -8.3592e-02\n",
      "  1.1632e-01  7.5145e-01  6.5186e-02 -3.3258e-01  1.4249e-01 -6.5348e-01\n",
      " -2.6661e-03  1.0233e-01 -3.8731e-01  6.0529e-01 -2.7326e-01 -3.5777e-01\n",
      " -5.3533e-01  4.9589e-02  2.0224e-01 -3.2805e-01  3.9305e-01 -2.7540e-01\n",
      " -1.3042e-01  9.5833e-01  3.8853e-01  5.9884e-01 -3.0503e-01 -1.0708e-01\n",
      "  4.5670e-01  7.2182e-01 -4.4221e-02 -6.8201e-02 -2.9903e-01  2.6483e-01\n",
      "  6.6810e-03  1.4919e-01 -5.3155e-01 -6.1697e-01 -5.4271e-01 -4.1103e-01\n",
      " -1.7534e-01  1.3229e-01  2.6958e-02 -1.5111e-01 -1.6844e-01 -3.0684e-01\n",
      " -5.1189e-02  9.2940e-01 -6.0882e-01 -1.1373e-01 -4.2237e-01 -4.0481e-01\n",
      " -3.4182e-01  3.8472e-01 -1.1566e-01  2.2679e-02  4.4793e-01  1.8849e-01\n",
      " -1.9598e-01 -6.8914e-02 -4.4520e-01  4.1643e-01  5.8319e-02 -3.9575e-01\n",
      " -4.0471e-01  4.2855e-01  1.6339e-01 -1.6965e-01 -5.7793e-02 -1.0358e-01\n",
      "  1.0223e+00  9.0564e-01 -5.5851e-01 -2.4856e-01  7.5582e-02 -2.7374e-01\n",
      "  3.8838e-01  5.1561e-01  6.0029e-02 -5.1990e-02  1.0113e+00  8.3607e-03\n",
      "  4.9764e-01  3.2839e-02  2.5892e-02  4.0404e-01 -3.7393e-01  1.3384e-01\n",
      " -2.9458e-01  3.4668e-01  5.2114e-02  1.0785e-01 -8.4700e-01  4.0936e-01\n",
      "  3.8801e-01  4.5079e-01 -4.3880e-01  1.9777e-01  1.9102e-01 -7.6580e-02\n",
      "  2.5788e-01  3.5642e-01  2.3305e-01 -1.6206e-01 -5.0847e-02  1.3708e-01\n",
      "  5.1338e-02 -3.8552e-01 -2.5497e-01  5.3901e-02 -6.2922e-01  3.2780e-01\n",
      " -2.0383e-01 -1.7265e-01 -9.9668e-02  2.4485e-01  3.2700e-02  3.2325e-01\n",
      "  2.3912e-01  7.7014e-03  1.2890e+00  6.8030e-02  2.0671e-02  2.6268e-01\n",
      "  1.9434e-01  5.6238e-01  7.1863e-02 -6.5502e-02 -2.2858e-01  9.2883e-02\n",
      "  3.9936e-01 -2.8038e-01  3.5633e-01  1.5203e-01 -2.7883e-01  3.8225e-02\n",
      " -3.9636e-01  3.2488e-01  2.4332e-01  2.2621e-01  2.3518e-01 -6.4747e-02\n",
      "  6.2858e-01 -1.1848e-01  1.2710e-01 -1.2697e-01  1.0015e+00 -2.5693e-01\n",
      "  2.0450e-01 -1.8789e-01 -1.2913e-01 -9.6456e-02  1.2301e-01 -1.3377e-01\n",
      " -1.2808e-01  4.1857e-02  8.6418e-01  3.6137e-01  9.0372e-02 -1.0687e-02\n",
      "  4.4736e-01  7.8975e-02 -3.2992e-01 -1.4470e-01 -6.1296e-01 -4.1543e-01\n",
      " -2.6104e-01 -8.4169e-01  4.7352e-01 -4.7640e-01 -3.4393e-01  8.2156e-02\n",
      " -2.4477e-01  7.9361e-01  2.6183e-01  4.6648e-01  1.9135e-01 -5.5033e-02\n",
      "  2.3492e-01  1.1178e-01 -3.7291e-01  2.6764e-01  1.1074e-01  6.8041e-01\n",
      " -1.0361e-03 -4.2537e-02  9.4006e-01 -2.8939e-01 -2.0841e-01  3.3001e-01\n",
      "  7.3318e-02 -5.1902e-01 -3.6412e-01 -4.1372e-01  2.0266e-01  4.3162e-01\n",
      " -1.0174e+00  2.5093e-01  3.8136e-01 -1.4332e-01 -2.0525e-01 -1.8724e-01\n",
      "  5.9925e-01  3.5604e-01 -9.1457e-01 -2.2401e-01 -4.2382e-02 -1.3299e-01\n",
      "  6.1671e-01  4.4886e-01 -1.1361e-01  2.0483e-01  2.5485e-01  2.7581e-01\n",
      " -8.7158e-01 -1.3156e-01  2.6117e-01  1.5815e-01 -3.2199e-01  7.1042e-01]\n",
      "jumps:  [-0.16814   -0.10948    0.2896    -0.21108   -0.29061    0.31201\n",
      "  0.04039   -0.10149   -0.18526   -0.55483   -0.36055   -0.093569\n",
      "  0.77334    0.027921   0.13389   -0.1014    -0.06482    0.24753\n",
      " -0.068026   0.26147   -0.14252    0.18657    0.030249  -0.07934\n",
      "  0.87696    0.61781    0.36835   -0.07306   -0.19303    0.3721\n",
      " -0.77087   -0.0062782 -0.19233   -0.43884   -0.79847    0.066636\n",
      " -0.21387   -0.65263   -0.073964   0.64115   -0.52014   -0.05981\n",
      " -0.1692    -0.413      0.060197   0.16327    0.43353    0.070021\n",
      "  0.063543   0.28527   -0.43474    0.15441    0.098037   0.098685\n",
      " -0.3965     0.38171   -0.084065  -0.4813    -0.59213    0.40444\n",
      " -0.2026     0.45569    0.039036  -0.41786   -0.20322   -0.11932\n",
      "  0.23747    0.26336    0.17139    0.12521   -0.55276    0.45515\n",
      " -0.63826    0.14054    0.35333   -0.28417    0.3889    -0.13004\n",
      " -0.027142  -0.23109    0.034327  -0.10685    0.85855    0.15145\n",
      "  0.16814    0.2281     0.22235   -0.1825    -0.019222   0.026105\n",
      "  0.47734    0.42115   -0.087767  -0.17439    0.22166   -0.36831\n",
      " -1.069      0.40489   -0.31038   -0.21588   -0.7282     0.29296\n",
      " -0.42949    0.23485    0.020585  -0.47795   -0.20216   -0.22146\n",
      " -0.45778    0.032547  -0.13727   -0.48945   -0.58148    0.051203\n",
      " -0.065926   0.46718   -0.080438  -0.25042   -0.4015     0.40254\n",
      "  0.12       0.5246     0.21582    0.1333     0.27662    0.2163\n",
      " -0.28177    0.67185   -0.025996  -0.40781   -0.23629    0.97455\n",
      " -0.51452    0.22697   -0.34857   -0.55928    0.019104  -0.016163\n",
      " -0.22626    0.094269   0.10808   -0.018804  -0.080299   0.0058964\n",
      " -0.61886    0.44842    0.2494    -0.25172    0.6705     0.23657\n",
      " -0.17631    0.28634   -0.39527   -0.22096    0.23069    0.0644\n",
      "  0.13151   -0.030479   0.38503   -0.084794   0.66178   -0.34578\n",
      "  0.3868     0.31506    0.20155   -0.026189   0.051188   0.16359\n",
      " -0.37096    0.21546   -0.19758   -0.083407   0.48437   -0.5487\n",
      "  0.35188   -0.43539    0.3976    -0.026514  -0.14485   -0.11205\n",
      "  0.59835    0.38055    0.1582    -0.24293    0.39837    0.44276\n",
      "  0.36113   -0.2358    -0.32568    0.54672   -0.36544   -0.33871\n",
      "  0.2154    -0.36877   -0.18857    0.42323   -0.56938   -0.20276\n",
      "  0.35452   -0.0167     1.157      0.22296   -0.35115    0.3662\n",
      " -0.20903    0.73497    0.018414   0.27861    0.044337   0.14504\n",
      " -0.23001   -0.54025   -0.26259   -0.94587    0.22239    0.44651\n",
      " -0.46075   -0.42224    0.0022315 -0.25522   -0.13135    0.65923\n",
      "  0.21267   -0.26498   -0.16777   -0.047013  -0.18551    0.12607\n",
      "  0.43205    0.153     -0.33748    0.010732  -0.18332   -0.28507\n",
      "  0.23396    0.3868    -0.34724    0.41692    0.56315    0.32366\n",
      "  0.55303    0.02256   -0.48281    0.54561   -0.51086   -0.55446\n",
      " -0.018992   0.62551    0.11436   -0.10939   -0.61894   -0.18419\n",
      "  0.27755   -0.29704   -0.21562   -0.011765   0.051859   0.04041\n",
      " -0.16798    0.65697   -0.36868    0.068085   0.34986    0.076252\n",
      "  0.12035    0.16001   -0.0088939  0.14892   -0.70374   -0.031891\n",
      "  0.2803    -0.26916   -0.61974    0.67423    0.080773  -0.32835\n",
      " -0.276     -0.60342   -0.60185    0.10982   -0.20312    0.42309\n",
      "  0.63701    0.51885   -0.31899    0.34256   -0.035933  -0.049542\n",
      " -0.89278   -0.03686    0.12862    0.41469   -0.37262   -0.20523\n",
      " -0.02476   -0.11959   -0.12129    0.099622   0.24538   -0.026313 ]\n",
      "over:  [-8.8137e-02 -2.1696e-02  2.9863e-01 -1.8325e-02 -2.3575e-01  1.1022e-01\n",
      " -1.7493e-01  9.9241e-03  2.3832e-01 -1.7643e+00  2.2489e-01  4.0552e-01\n",
      " -4.8176e-01 -6.6099e-02  1.3290e-01  4.7502e-01 -5.6438e-02  3.2902e-01\n",
      "  9.7628e-02  4.2467e-01  2.5285e-01 -1.7258e-01  7.6564e-02 -1.5678e-01\n",
      " -2.2694e-01 -2.1213e-01 -3.3460e-01  7.3842e-02 -4.4671e-01  3.3979e-01\n",
      " -2.3534e-01  1.5013e-01 -3.4718e-01  5.4379e-02 -8.8699e-01 -3.5534e-01\n",
      " -1.3166e-01 -1.8265e-02 -1.8708e-01  3.0193e-01  1.0008e-01 -1.1980e-01\n",
      " -7.4867e-01  1.5592e-01  4.2757e-01 -2.9996e-01 -1.2499e-01  1.2750e-01\n",
      " -2.1962e-01  4.5077e-01 -3.2268e-01 -2.8934e-01 -1.3745e-01 -1.7738e-01\n",
      "  2.4185e-01 -1.2240e-02  1.7264e-01  2.7287e-01 -8.2743e-02  2.0570e-01\n",
      "  2.2559e-02 -7.5793e-02  2.1027e-01 -3.1239e-01  1.3032e-01 -7.5693e-01\n",
      "  9.0872e-02  4.9975e-01  1.1746e-01 -5.3133e-01 -8.5348e-02  4.4042e-01\n",
      "  1.2231e-01 -1.8672e-01  1.1668e-01  1.0322e-02 -2.6779e-01  1.5042e-01\n",
      " -5.4751e-01 -3.8439e-01 -6.7117e-01 -3.4912e-01  1.4190e-01  1.2742e-01\n",
      " -1.2879e-01 -1.3582e-01  2.9059e-01  1.4339e-01  2.8467e-01  7.0642e-02\n",
      "  4.7118e-03  1.8321e-01 -4.0973e-01  2.1973e-01 -6.6693e-01 -1.0599e-01\n",
      " -3.4465e-01  2.3103e-01 -3.6278e-01 -1.2365e-02  1.1544e-01  3.6134e-02\n",
      "  7.4889e-02 -6.4933e-01 -3.3362e-02 -3.0498e-01  4.0964e-01  2.6898e-01\n",
      " -2.1150e-01 -1.1446e-01 -4.8326e-02 -5.9265e-01 -3.7301e-01 -5.4902e-02\n",
      " -1.9869e-01  2.3181e-01 -2.9017e-01  6.2177e-01  8.9856e-02 -6.4116e-01\n",
      "  3.9304e-02  9.7170e-02 -2.9954e-01  4.0426e-01  8.4010e-02  1.2902e-01\n",
      " -3.5864e-01  3.9975e-01  1.3183e-01 -1.4797e-01 -3.1449e-01  4.8654e-01\n",
      "  2.9974e-02  5.3637e-01  1.1619e-01  1.4643e-01 -2.2927e-01  7.9999e-02\n",
      "  1.3114e-01 -3.0946e-02  1.6410e-01 -3.5597e-01  2.0801e-01  3.3193e-01\n",
      " -8.4092e-01  3.3762e-01 -7.5518e-02 -4.3158e-01  1.9592e-01 -2.7815e-01\n",
      "  6.6744e-01  3.1189e-02 -1.1706e-02  5.3737e-01  2.9596e-01  3.7335e-01\n",
      " -2.2166e-01  2.3843e-02 -1.1905e-01 -1.8529e-01  1.1857e-02 -2.3540e-01\n",
      "  2.3800e-01 -3.1987e-02 -1.6400e-01 -7.9319e-02  9.9570e-02  2.6182e-02\n",
      " -6.0793e-01 -6.7515e-02  3.9870e-02  7.0721e-02 -6.0965e-01 -7.3549e-01\n",
      " -8.1241e-03 -1.3363e-01 -1.6572e-01  3.2373e-01  1.0115e-01  6.0659e-01\n",
      " -2.2547e-02  2.5960e-01  2.4251e-01 -7.8426e-02 -7.8699e-02 -4.1680e-01\n",
      " -1.4208e-01  2.5136e-02 -3.4187e-01  2.7631e-01  2.4892e-01  2.7898e-01\n",
      " -2.6522e-01 -3.3989e-01  1.7929e-01 -4.0393e-01  2.7421e-01  2.2639e-01\n",
      "  4.5234e-01 -2.4806e-01  9.4945e-01  1.4040e-01  2.3117e-02 -3.3321e-01\n",
      "  3.6958e-01 -1.7507e-01 -1.8646e-01 -2.4238e-01  2.0633e-02 -7.8795e-02\n",
      "  5.1631e-01  4.5651e-01  2.5398e-01  3.8711e-01  1.6812e-01 -5.8263e-01\n",
      " -9.9027e-02 -2.3445e-01  3.6928e-02  6.1330e-02  2.4186e-01 -2.0908e-01\n",
      " -3.8720e-02  1.4625e-01 -3.6779e-02  2.5993e-01  3.1897e-01 -5.5307e-02\n",
      " -7.7177e-02  3.0117e-01  2.3307e-01  4.1980e-01 -1.3339e-01 -2.0384e-02\n",
      "  7.1158e-01  8.4989e-02  3.2791e-01  4.8060e-01 -3.3761e-01  5.7392e-02\n",
      " -2.6536e-01  3.0228e-01  1.8540e-01 -8.3752e-02 -7.3776e-01 -1.4404e-03\n",
      "  3.4121e-01  2.9959e-02  4.2035e-01  7.3122e-02 -1.9430e-01 -2.2341e-01\n",
      "  5.8658e-01 -2.2756e-01  6.3029e-01 -6.8682e-02 -3.5365e-01  2.6851e-01\n",
      " -7.3124e-02  1.5702e-02  1.2022e-01 -2.4631e-01  2.8471e-01  3.8292e-01\n",
      " -3.8869e-01  2.0974e-01 -2.0512e-01 -1.8508e-01  3.2543e-01  3.7336e-03\n",
      " -1.9901e-02 -8.4242e-02  2.9326e-01 -5.8162e-02  3.6575e-01  4.6895e-02\n",
      " -2.1328e+00 -3.5109e-01  4.1716e-01  1.8393e-01 -3.0990e-01  2.7303e-01\n",
      "  1.6627e-02  4.4978e-03 -7.2902e-02  5.3814e-02 -5.7213e-02  1.5815e-01\n",
      "  1.4517e-01  1.6580e-01 -7.3557e-02  1.6125e-01 -9.9237e-02  2.6099e-01\n",
      "  2.7941e-02  3.2140e-01  1.6931e-01 -3.5469e-01 -4.2713e-01 -3.9323e-01]\n",
      "the:  [ 4.6560e-02  2.1318e-01 -7.4364e-03 -4.5854e-01 -3.5639e-02  2.3643e-01\n",
      " -2.8836e-01  2.1521e-01 -1.3486e-01 -1.6413e+00 -2.6091e-01  3.2434e-02\n",
      "  5.6621e-02 -4.3296e-02 -2.1672e-02  2.2476e-01 -7.5129e-02 -6.7018e-02\n",
      " -1.4247e-01  3.8825e-02 -1.8951e-01  2.9977e-01  3.9305e-01  1.7887e-01\n",
      " -1.7343e-01 -2.1178e-01  2.3617e-01 -6.3681e-02 -4.2318e-01 -1.1661e-01\n",
      "  9.3754e-02  1.7296e-01 -3.3073e-01  4.9112e-01 -6.8995e-01 -9.2462e-02\n",
      "  2.4742e-01 -1.7991e-01  9.7908e-02  8.3118e-02  1.5299e-01 -2.7276e-01\n",
      " -3.8934e-02  5.4453e-01  5.3737e-01  2.9105e-01 -7.3514e-03  4.7880e-02\n",
      " -4.0760e-01 -2.6759e-02  1.7919e-01  1.0977e-02 -1.0963e-01 -2.6395e-01\n",
      "  7.3990e-02  2.6236e-01 -1.5080e-01  3.4623e-01  2.5758e-01  1.1971e-01\n",
      " -3.7135e-02 -7.1593e-02  4.3898e-01 -4.0764e-02  1.6425e-02 -4.4640e-01\n",
      "  1.7197e-01  4.6246e-02  5.8639e-02  4.1499e-02  5.3948e-01  5.2495e-01\n",
      "  1.1361e-01 -4.8315e-02 -3.6385e-01  1.8704e-01  9.2761e-02 -1.1129e-01\n",
      " -4.2085e-01  1.3992e-01 -3.9338e-01 -6.7945e-02  1.2188e-01  1.6707e-01\n",
      "  7.5169e-02 -1.5529e-02 -1.9499e-01  1.9638e-01  5.3194e-02  2.5170e-01\n",
      " -3.4845e-01 -1.0638e-01 -3.4692e-01 -1.9024e-01 -2.0040e-01  1.2154e-01\n",
      " -2.9208e-01  2.3353e-02 -1.1618e-01 -3.5768e-01  6.2304e-02  3.5884e-01\n",
      "  2.9060e-02  7.3005e-03  4.9482e-03 -1.5048e-01 -1.2313e-01  1.9337e-01\n",
      "  1.2173e-01  4.4503e-01  2.5147e-01  1.0781e-01 -1.7716e-01  3.8691e-02\n",
      "  8.1530e-02  1.4667e-01  6.3666e-02  6.1332e-02 -7.5569e-02 -3.7724e-01\n",
      "  1.5850e-02 -3.0342e-01  2.8374e-01 -4.2013e-02 -4.0715e-02 -1.5269e-01\n",
      "  7.4980e-02  1.5577e-01  1.0433e-01  3.1393e-01  1.9309e-01  1.9429e-01\n",
      "  1.5185e-01 -1.0192e-01 -1.8785e-02  2.0791e-01  1.3366e-01  1.9038e-01\n",
      " -2.5558e-01  3.0400e-01 -1.8960e-02  2.0147e-01 -4.2110e-01 -7.5156e-03\n",
      " -2.7977e-01 -1.9314e-01  4.6204e-02  1.9971e-01 -3.0207e-01  2.5735e-01\n",
      "  6.8107e-01 -1.9409e-01  2.3984e-01  2.2493e-01  6.5224e-01 -1.3561e-01\n",
      " -1.7383e-01 -4.8209e-02 -1.1860e-01  2.1588e-03 -1.9525e-02  1.1948e-01\n",
      "  1.9346e-01 -4.0820e-01 -8.2966e-02  1.6626e-01 -1.0601e-01  3.5861e-01\n",
      "  1.6922e-01  7.2590e-02 -2.4803e-01 -1.0024e-01 -5.2491e-01 -1.7745e-01\n",
      " -3.6647e-01  2.6180e-01 -1.2077e-02  8.3190e-02 -2.1528e-01  4.1045e-01\n",
      "  2.9136e-01  3.0869e-01  7.8864e-02  3.2207e-01 -4.1023e-02 -1.0970e-01\n",
      " -9.2041e-02 -1.2339e-01 -1.6416e-01  3.5382e-01 -8.2774e-02  3.3171e-01\n",
      " -2.4738e-01 -4.8928e-02  1.5746e-01  1.8988e-01 -2.6642e-02  6.3315e-02\n",
      " -1.0673e-02  3.4089e-01  1.4106e+00  1.3417e-01  2.8191e-01 -2.5940e-01\n",
      "  5.5267e-02 -5.2425e-02 -2.5789e-01  1.9127e-02 -2.2084e-02  3.2113e-01\n",
      "  6.8818e-02  5.1207e-01  1.6478e-01 -2.0194e-01  2.9232e-01  9.8575e-02\n",
      "  1.3145e-02 -1.0652e-01  1.3510e-01 -4.5332e-02  2.0697e-01 -4.8425e-01\n",
      " -4.4706e-01  3.3305e-03  2.9264e-03 -1.0975e-01 -2.3325e-01  2.2442e-01\n",
      " -1.0503e-01  1.2339e-01  1.0978e-01  4.8994e-02 -2.5157e-01  4.0319e-01\n",
      "  3.5318e-01  1.8651e-01 -2.3622e-02 -1.2734e-01  1.1475e-01  2.7359e-01\n",
      " -2.1866e-01  1.5794e-02  8.1754e-01 -2.3792e-02 -8.5469e-01 -1.6203e-01\n",
      "  1.8076e-01  2.8014e-02 -1.4340e-01  1.3139e-03 -9.1735e-02 -8.9704e-02\n",
      "  1.1105e-01 -1.6703e-01  6.8377e-02 -8.7388e-02 -3.9789e-02  1.4184e-02\n",
      "  2.1187e-01  2.8579e-01 -2.8797e-01 -5.8996e-02 -3.2436e-02 -4.7009e-03\n",
      " -1.7052e-01 -3.4741e-02 -1.1489e-01  7.5093e-02  9.9526e-02  4.8183e-02\n",
      " -7.3775e-02 -4.1817e-01  4.1268e-03  4.4414e-01 -1.6062e-01  1.4294e-01\n",
      " -2.2628e+00 -2.7347e-02  8.1311e-01  7.7417e-01 -2.5639e-01 -1.1576e-01\n",
      " -1.1982e-01 -2.1363e-01  2.8429e-02  2.7261e-01  3.1026e-02  9.6782e-02\n",
      "  6.7769e-03  1.4082e-01 -1.3064e-02 -2.9686e-01 -7.9913e-02  1.9500e-01\n",
      "  3.1549e-02  2.8506e-01 -8.7461e-02  9.0611e-03 -2.0989e-01  5.3913e-02]\n",
      "lazy:  [ 4.2791e-01 -1.6070e-01  2.4912e-01  3.9763e-01 -3.2224e-01  1.9783e-04\n",
      "  8.8576e-02  4.0501e-01 -2.4655e-01  1.6410e-02 -2.3331e-01 -1.2307e-01\n",
      " -3.9679e-01  2.1877e-02  1.6211e-01  3.8852e-01  2.5025e-01 -4.3968e-02\n",
      "  1.0819e+00  6.6517e-01  2.1829e-01  7.9898e-01 -6.1695e-01 -1.0184e-01\n",
      "  2.2900e-01 -3.1982e-01  3.1205e-01 -1.0453e-01  4.4810e-01  3.6708e-02\n",
      " -1.1376e-02  7.0543e-01  2.7454e-01  2.5835e-01 -5.1821e-01  8.2996e-01\n",
      " -8.0672e-02 -4.5489e-01 -3.4843e-01  5.1421e-01 -5.7408e-01  6.4612e-01\n",
      " -2.1564e-01 -5.4848e-01  2.3656e-01  1.8453e-02  8.6403e-01 -1.0357e-01\n",
      " -5.6097e-02 -4.8564e-01  2.7927e-01 -6.1733e-01  5.2877e-01 -1.9221e-01\n",
      " -1.4684e-01  4.2272e-01  5.6312e-02 -3.3168e-01  8.1864e-02  2.2225e-01\n",
      "  2.7757e-01 -2.1934e-02  2.8499e-01  5.7453e-03 -4.0598e-01  6.1549e-02\n",
      "  1.1544e-01  7.2053e-02 -5.4945e-02 -3.9549e-02 -1.9371e-01  2.7872e-01\n",
      " -1.0139e-01 -7.1846e-02 -3.4043e-01 -2.2490e-02 -2.6097e-03  6.5403e-01\n",
      "  4.1425e-01 -1.1459e-01 -4.9802e-01 -3.4516e-02 -3.0815e-02 -6.1508e-01\n",
      "  4.8955e-01 -1.0641e-01  6.3485e-02  5.5039e-01 -1.6282e-01 -7.7526e-02\n",
      " -1.5945e-01  3.0791e-01 -3.0439e-02 -3.8283e-01  7.8436e-02  1.1488e-01\n",
      "  6.7573e-02  2.2181e-01 -1.4319e-02  1.4366e-02  2.8839e-01 -9.1358e-01\n",
      " -5.3084e-01  1.3097e-01 -2.0027e-01  2.1495e-01 -3.6158e-01  7.6012e-02\n",
      "  1.2718e-01  2.5851e-01  6.6530e-02  3.1628e-01 -6.3175e-01 -4.1942e-01\n",
      "  3.8640e-01  7.3017e-02 -9.2298e-02 -8.8510e-01 -2.1618e-01  2.9006e-01\n",
      "  3.6404e-01  2.3740e-01 -2.2910e-01  3.2436e-01  6.0954e-01  3.2458e-01\n",
      " -8.4691e-02  4.1471e-01  2.6053e-01  6.8716e-02 -4.4528e-01  2.9296e-02\n",
      " -4.7913e-02  4.5709e-01 -5.2956e-01 -3.0998e-01  3.2488e-01 -3.6054e-01\n",
      "  1.8449e-01 -3.8492e-01  3.9918e-02  4.0046e-01 -8.7039e-02 -4.9739e-01\n",
      " -7.1877e-01  1.4894e-01  1.5550e-01  3.2388e-01  6.7209e-01  4.0215e-01\n",
      " -7.3436e-01  7.0311e-02 -7.3754e-02  3.1062e-01  1.4725e-03  4.3386e-02\n",
      "  2.9529e-01 -1.0544e-01  7.5028e-01  2.2274e-01  4.3789e-02 -6.7316e-01\n",
      "  6.8448e-01 -5.5239e-01 -2.2204e-02 -1.8850e-01 -4.0747e-01 -5.7321e-02\n",
      "  1.8211e-01 -5.8300e-01 -2.0733e-01  2.8411e-01 -3.5269e-01 -1.2422e-03\n",
      " -1.7660e-01 -6.9418e-01 -2.7758e-01  3.6022e-01 -1.7472e-01 -2.9792e-01\n",
      "  4.5945e-01  1.3176e-01 -2.2119e-01 -1.0709e-01  1.6470e-02  1.5636e-01\n",
      "  4.0296e-01 -7.1640e-01  4.4003e-01 -2.0855e-01 -5.1675e-01  3.3292e-01\n",
      "  2.5018e-01 -5.7151e-01 -7.5581e-04 -8.8453e-02 -4.6790e-01 -6.3341e-01\n",
      "  1.7374e-01 -1.2076e-01  5.9592e-01 -2.6351e-02 -1.7858e-02  1.2978e-01\n",
      "  2.4669e-01  9.1323e-02 -1.4112e-01  4.6466e-02  1.8030e-01  1.7382e-01\n",
      " -7.7972e-02 -1.4600e-01 -5.1364e-01  5.1154e-01 -3.3271e-01  5.2346e-01\n",
      " -9.4829e-02 -2.0365e-01  5.6919e-01 -1.5709e-01 -5.4340e-01  2.4034e-01\n",
      " -4.5061e-02  1.5918e-01 -7.0530e-01 -7.9981e-03  5.1987e-01 -1.6802e-01\n",
      " -8.0854e-03  2.4719e-01  3.6062e-01 -2.2302e-01 -3.2196e-01 -7.6371e-01\n",
      "  1.9203e-02  2.0398e-01 -4.4568e-01  1.1332e-01 -1.3784e-01 -5.6301e-02\n",
      "  5.5306e-01  4.6183e-01 -8.0710e-01 -4.1624e-01 -5.1206e-01 -8.1953e-01\n",
      "  7.8391e-03  2.8204e-02  2.0151e-01  4.5986e-01  2.0020e-01 -9.1094e-02\n",
      "  4.3782e-01  3.4559e-01  3.6562e-01  3.4960e-01  1.7984e-01 -2.3978e-01\n",
      " -2.5039e-01  6.7002e-03  1.0974e-01  8.1626e-02 -2.4783e-01  2.6453e-01\n",
      " -3.6779e-02  3.1099e-01  6.7982e-01 -4.6699e-01 -2.8060e-01  8.2703e-01\n",
      "  2.3553e-01 -7.4127e-01  2.9891e-02 -1.3198e-01  2.2106e-01  1.7262e-01\n",
      " -4.2037e-01  5.6484e-01 -7.0211e-01 -1.5537e-01 -8.0067e-02 -6.9698e-02\n",
      "  8.0176e-01 -2.4841e-01 -7.1711e-01 -2.5340e-01  7.2812e-01  1.6527e-01\n",
      " -2.2780e-01 -3.2008e-01  2.8609e-01 -5.8733e-02 -5.4448e-01  2.6380e-01\n",
      "  3.3366e-01 -5.1100e-01 -1.0377e-01 -1.9413e-01 -3.9855e-01  2.2238e-01]\n",
      "dog:  [-1.1043e-01  8.1217e-01  7.3668e-02  1.9023e-01 -5.2888e-02  6.1468e-02\n",
      "  1.6076e-01  4.1302e-01 -3.0199e-01 -9.0827e-01  2.7504e-01 -3.1890e-02\n",
      " -2.8842e-01  2.3447e-01  4.7679e-01  5.0124e-01  2.9371e-01  2.7029e-01\n",
      "  5.4745e-02  9.8038e-02  5.7116e-01  3.6755e-01  4.0734e-02  3.4347e-01\n",
      " -1.8256e-01 -2.8935e-01  2.3826e-02 -1.9401e-01  2.4444e-01  1.3407e-01\n",
      " -1.6494e-01 -2.6983e-01 -2.6234e-01 -2.1779e-01 -8.7528e-01  7.3822e-01\n",
      " -8.7931e-02 -1.0876e-02 -2.6540e-01  3.4668e-01 -5.5814e-01  1.7591e-01\n",
      "  1.6926e-01 -1.5725e-01 -5.0430e-01 -2.0100e-01  6.6701e-01 -3.2518e-02\n",
      "  4.5012e-02  6.5675e-02 -1.6061e-01 -7.3363e-01  2.4642e-01  3.4325e-01\n",
      "  2.1899e-01  4.8646e-02 -5.9987e-01 -5.8153e-02 -5.1694e-02 -5.7846e-01\n",
      "  3.0000e-01  3.5078e-01  4.6646e-01 -7.5309e-03  1.0455e-01 -5.1016e-01\n",
      " -5.5987e-02 -1.0295e-01 -2.6476e-01 -4.1230e-02 -2.8371e-02  5.1979e-01\n",
      " -3.4849e-01 -4.7217e-01 -3.7229e-01 -3.2790e-02  1.3989e-01  3.5716e-01\n",
      "  1.9305e-01 -2.1986e-01  2.4136e-01  4.0976e-01  3.7516e-01  1.4255e-01\n",
      " -3.4143e-02 -7.2653e-01 -1.0832e-01  6.8616e-01 -2.6335e-01 -4.2345e-01\n",
      " -2.4253e-01  1.5778e-01  1.4258e-01 -3.2749e-01 -3.4699e-01  1.6148e-01\n",
      "  1.9603e-01  4.1639e-01 -2.3370e-01  7.5816e-02  1.5899e-01  1.6623e-03\n",
      " -4.8301e-02 -1.0611e-01 -1.9326e-01  1.4494e-01  1.5406e-02  1.0629e-01\n",
      " -3.6699e-02  6.3230e-01  1.2986e-01  4.9902e-01 -1.1323e+00 -1.2636e-01\n",
      "  6.4718e-02  1.2374e-01 -4.9712e-01 -1.4836e-02  1.0488e-01 -4.9818e-01\n",
      " -2.8856e-01  3.8949e-01 -3.1828e-02 -2.8625e-01 -9.8758e-02 -7.6990e-02\n",
      " -2.4234e-01  7.5793e-01  3.4835e-01 -7.1030e-01  4.5318e-01 -3.4418e-01\n",
      " -1.9459e-01  6.1478e-01 -2.9010e-02 -2.7864e-01  3.8556e-01  1.0072e-01\n",
      "  1.2895e-01  1.7992e-02  3.3670e-01  2.0698e-01 -3.8049e-01 -6.6661e-03\n",
      "  1.1540e-01 -8.5268e-02 -1.4608e-01  4.4514e-01 -9.3674e-02  2.3639e-01\n",
      " -1.1447e-01  1.0948e+00 -5.7823e-02 -1.6295e-01  5.5880e-01 -1.8988e-02\n",
      " -7.1374e-02  2.1319e-01  6.1277e-02  7.2759e-01  6.2747e-01 -1.9280e-01\n",
      "  1.3057e-01  1.7426e-01 -1.0229e-01  1.5232e-01  5.2500e-01 -2.1919e-01\n",
      " -2.7185e-01 -5.4186e-01  3.1752e-01  1.6375e-01 -2.9039e-01  1.7074e-01\n",
      " -3.1814e-01 -9.6421e-01 -1.1610e-01 -2.9951e-01  1.8686e-01 -4.5986e-01\n",
      "  4.1633e-01 -1.7583e-01 -3.4583e-01 -2.7244e-01 -5.0216e-01  1.2852e-02\n",
      "  5.9838e-01 -1.1237e-01  2.4697e-01 -4.9048e-01 -4.4188e-01 -1.6255e-01\n",
      " -7.3313e-01 -3.7677e-01 -6.8925e-01  6.1174e-02 -4.2101e-01 -1.3153e-01\n",
      " -8.3590e-03 -1.8360e-02  1.3686e+00  4.6169e-02  9.4622e-01 -1.5126e-02\n",
      " -1.2477e-01  4.8754e-01  2.2384e-01 -2.1820e-01 -2.3389e-01  1.5207e-01\n",
      " -2.8718e-01 -6.3908e-01 -2.2383e-01 -1.8014e-01 -3.3548e-01  5.3587e-01\n",
      " -2.9367e-01  1.0866e-01  6.3411e-02 -9.3424e-03 -1.5886e-01  2.2602e-01\n",
      "  1.1925e-01 -4.1442e-01 -7.8062e-02 -9.7857e-02  2.7938e-01 -1.8348e-01\n",
      " -3.4584e-01  1.8489e-01  1.7402e-01 -5.2198e-01 -4.3306e-01  1.6256e-01\n",
      "  1.4032e-01  3.5124e-01 -1.8280e-01 -3.5984e-01 -1.3009e-01  1.6304e-01\n",
      "  3.1734e-01  3.7716e-03 -4.5498e-02 -4.2066e-01 -4.4419e-01 -6.8985e-01\n",
      " -4.9359e-01  7.0281e-02 -1.4377e-01  6.2508e-01 -5.6311e-02  1.8850e-01\n",
      " -5.6785e-02  1.4052e-01  1.1973e+00  7.1894e-01  5.4332e-01 -1.2461e-01\n",
      " -1.1978e-01  3.0163e-01 -1.6273e-01 -4.6740e-02 -2.5249e-01 -3.0659e-02\n",
      " -3.2271e-01  3.2361e-01  3.3244e-01 -2.7819e-02 -3.3367e-01 -2.3444e-02\n",
      " -5.0394e-01 -2.0587e-01 -1.3013e-01 -3.5884e-01  4.5384e-02 -1.1863e-01\n",
      " -1.7257e+00  3.9441e-01 -5.3179e-01  5.8209e-01 -6.5771e-01  3.6849e-01\n",
      "  2.3518e-01  1.0802e-01 -8.3159e-01  6.1486e-01  2.5547e-01 -4.5289e-01\n",
      "  5.1446e-01 -1.7911e-01 -1.2389e-01  1.8688e-01 -4.1102e-01 -7.0877e-01\n",
      " -3.7501e-01 -6.6152e-01  6.7730e-01  3.3936e-01  5.7994e-01  6.8149e-02]\n"
     ]
    }
   ],
   "source": [
    "# Verify the new vocabulary: should get same embeddings for test sentence\n",
    "# Note that a small EVOCABSIZE may yield some zero vectors for embeddings\n",
    "print('\\nTest sentence embeddings from vocabulary of', EVOCABSIZE, 'words:\\n')\n",
    "for word in words_in_test_sentence:\n",
    "    word_ = word.lower()\n",
    "    embedding = limited_index_to_embedding[limited_word_to_index[word_]]\n",
    "    print(word_ + \": \", embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of lists of lists for embeddings\n",
    "embeddings = []    \n",
    "for doc in documents:\n",
    "    embedding = []\n",
    "    for word in doc:\n",
    "       embedding.append(limited_index_to_embedding[limited_word_to_index[word]]) \n",
    "    embeddings.append(embedding)\n",
    "\n",
    "# -----------------------------------------------------    \n",
    "# Make embeddings a numpy array for use in an RNN \n",
    "# Create training and test sets with Scikit Learn\n",
    "# -----------------------------------------------------\n",
    "embeddings_array = np.array(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random splitting of the data in to training (80%) and test (20%)  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings_array, thumbs_down_up, test_size=0.20, random_state = RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ---- Epoch  0  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.55 Test accuracy: 0.51\n",
      "\n",
      "  ---- Epoch  1  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.61 Test accuracy: 0.555\n",
      "\n",
      "  ---- Epoch  2  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.61 Test accuracy: 0.56\n",
      "\n",
      "  ---- Epoch  3  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.65 Test accuracy: 0.575\n",
      "\n",
      "  ---- Epoch  4  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.68 Test accuracy: 0.565\n",
      "\n",
      "  ---- Epoch  5  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.68 Test accuracy: 0.595\n",
      "\n",
      "  ---- Epoch  6  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.72 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  7  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.73 Test accuracy: 0.605\n",
      "\n",
      "  ---- Epoch  8  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.73 Test accuracy: 0.61\n",
      "\n",
      "  ---- Epoch  9  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.78 Test accuracy: 0.62\n",
      "\n",
      "  ---- Epoch  10  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.8 Test accuracy: 0.615\n",
      "\n",
      "  ---- Epoch  11  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.8 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  12  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.83 Test accuracy: 0.635\n",
      "\n",
      "  ---- Epoch  13  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.87 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  14  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.88 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  15  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.9 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  16  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.9 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  17  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.92 Test accuracy: 0.675\n",
      "\n",
      "  ---- Epoch  18  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.93 Test accuracy: 0.685\n",
      "\n",
      "  ---- Epoch  19  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.93 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  20  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.93 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  21  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.93 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  22  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.93 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  23  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.94 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  24  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.94 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  25  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.94 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  26  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.95 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  27  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.95 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  28  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.96 Test accuracy: 0.675\n",
      "\n",
      "  ---- Epoch  29  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.96 Test accuracy: 0.675\n",
      "\n",
      "  ---- Epoch  30  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.97 Test accuracy: 0.68\n",
      "\n",
      "  ---- Epoch  31  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.95 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  32  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.94 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  33  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.93 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  34  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Train accuracy: 0.96 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  35  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.98 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  36  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.98 Test accuracy: 0.68\n",
      "\n",
      "  ---- Epoch  37  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.96 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  38  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.96 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  39  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.97 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  40  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.98 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  41  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.69\n",
      "\n",
      "  ---- Epoch  42  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.67\n",
      "\n",
      "  ---- Epoch  43  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.68\n",
      "\n",
      "  ---- Epoch  44  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.97 Test accuracy: 0.655\n",
      "\n",
      "  ---- Epoch  45  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.98 Test accuracy: 0.64\n",
      "\n",
      "  ---- Epoch  46  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 1.0 Test accuracy: 0.65\n",
      "\n",
      "  ---- Epoch  47  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.66\n",
      "\n",
      "  ---- Epoch  48  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.665\n",
      "\n",
      "  ---- Epoch  49  ----\n",
      "\n",
      "  Batch  0  training observations from  0  to  99\n",
      "  Batch  1  training observations from  100  to  199\n",
      "  Batch  2  training observations from  200  to  299\n",
      "  Batch  3  training observations from  300  to  399\n",
      "  Batch  4  training observations from  400  to  499\n",
      "  Batch  5  training observations from  500  to  599\n",
      "  Batch  6  training observations from  600  to  699\n",
      "  Batch  7  training observations from  700  to  799\n",
      "\n",
      "  Train accuracy: 0.99 Test accuracy: 0.665\n",
      "Duration in minutes: 0.29\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_steps = embeddings_array.shape[1]  # number of words per document \n",
    "n_inputs = embeddings_array.shape[2]  # dimension of  pre-trained embeddings\n",
    "n_neurons = 20  # analyst specified number of neurons\n",
    "n_outputs = 2  # thumbs-down or thumbs-up\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Start timer\n",
    "start = time.clock()\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "\n",
    "logits = tf.layers.dense(states, n_outputs)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                          logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        print('\\n  ---- Epoch ', epoch, ' ----\\n')\n",
    "        for iteration in range(y_train.shape[0] // batch_size):          \n",
    "            X_batch = X_train[iteration*batch_size:(iteration + 1)*batch_size,:]\n",
    "            y_batch = y_train[iteration*batch_size:(iteration + 1)*batch_size]\n",
    "            print('  Batch ', iteration, ' training observations from ',  \n",
    "                  iteration*batch_size, ' to ', (iteration + 1)*batch_size-1,)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        print('\\n  Train accuracy:', acc_train, 'Test accuracy:', acc_test)\n",
    "        \n",
    "# Record the clock time it takes\n",
    "duration = round((time.clock() - start ) / 60, 2)\n",
    "print('Duration in minutes:', duration)\n",
    "\n",
    "#acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "\n",
    "metrics['Model 4'] = ['glove.6B.300d', n_epochs, EVOCABSIZE, duration, acc_train, acc_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark results for all models.\n",
    "\n",
    "In the below table, a summary of the models, their characteristics, and performances are shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Convert metrics dictionary to dataframe for display\n",
    "results_summary = pd.DataFrame.from_dict(metrics, orient='index') \n",
    "results_summary.columns = names\n",
    "# Sort by model number\n",
    "results_summary.reset_index(inplace=True) \n",
    "results_summary.sort_values(by=['index'], axis=0, inplace=True) \n",
    "results_summary.set_index(['index'], inplace=True) \n",
    "results_summary.index.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model description</th>\n",
       "      <th>Number of epochs</th>\n",
       "      <th>Vocabulary size</th>\n",
       "      <th>Time in minutes</th>\n",
       "      <th>Training accuracy</th>\n",
       "      <th>Test accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Model 1</th>\n",
       "      <td>glove.6B.50d</td>\n",
       "      <td>50</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model 2</th>\n",
       "      <td>glove.6B.50d</td>\n",
       "      <td>50</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model 3</th>\n",
       "      <td>glove.6B.300d</td>\n",
       "      <td>50</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model 4</th>\n",
       "      <td>glove.6B.300d</td>\n",
       "      <td>50</td>\n",
       "      <td>30000</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Model description  Number of epochs  Vocabulary size  Time in minutes  \\\n",
       "Model 1      glove.6B.50d                50            10000             0.12   \n",
       "Model 2      glove.6B.50d                50            30000             0.12   \n",
       "Model 3     glove.6B.300d                50            10000             0.31   \n",
       "Model 4     glove.6B.300d                50            30000             0.29   \n",
       "\n",
       "         Training accuracy  Test accuracy  \n",
       "Model 1               0.86          0.675  \n",
       "Model 2               0.88          0.655  \n",
       "Model 3               0.98          0.625  \n",
       "Model 4               0.99          0.665  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "The best models on the test set is Model 1. Very similar risult can be obtained also with Model 4.\n",
    "\n",
    "Increasing vocabulary size and or embeddings only result in overfitting. The RNN should be modified to adress the issue.  \n",
    "\n",
    "From this study we can deduct that is possible to use a very simple model that is able to predict the type of content (positive or negative), once the system is fed with proper historic data. The RNN should be improved to try to get a better accuracy and reduce the overfitting present in all the models.\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
